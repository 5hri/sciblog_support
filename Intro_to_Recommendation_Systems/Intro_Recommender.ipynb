{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Recommendation Systems\n",
    "\n",
    "In this tutorial we are going to use a [deep autoencoder](https://arxiv.org/abs/1708.01715) to perform collaborative filtering in the [Netflix dataset](https://netflixprize.com/). \n",
    "\n",
    "[Collaborative filtering](https://en.wikipedia.org/wiki/Collaborative_filtering) is one of the most pupular techniques in recommendation systems. It is based on inferring the missing entries in an `mxn` matrix, `R`, whose `(i, j)` entry describes the ratings given by the `ith` user to the `jth` item. The performance is then measured using Root\n",
    "Mean Squared Error (RMSE).\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"https://upload.wikimedia.org/wikipedia/commons/5/52/Collaborative_filtering.gif\" width=300px/>\n",
    "</p>\n",
    "\n",
    "The code in this tutorial is done with [PyTorch](http://pytorch.org/) and is based on [this repo](https://github.com/NVIDIA/DeepRecommender) by NVIDIA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OS:  linux\n",
      "Python:  3.5.4 | packaged by conda-forge | (default, Nov  4 2017, 10:11:29) \n",
      "[GCC 4.8.2 20140120 (Red Hat 4.8.2-15)]\n",
      "PyTorch:  0.3.0.post4\n",
      "Numpy:  1.14.0\n",
      "Number of CPU processors:  24\n",
      "GPU:  ['Tesla M60', 'Tesla M60', 'Tesla M60', 'Tesla M60']\n",
      "GPU memory:  ['8123 MiB', '8123 MiB', '8123 MiB', '8123 MiB']\n",
      "CUDA:  CUDA Version 8.0.61\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from utils import get_gpu_name, get_number_processors, get_gpu_memory, get_cuda_version\n",
    "from parameters import *\n",
    "\n",
    "print(\"OS: \", sys.platform)\n",
    "print(\"Python: \", sys.version)\n",
    "print(\"PyTorch: \", torch.__version__)\n",
    "print(\"Numpy: \", np.__version__)\n",
    "print(\"Number of CPU processors: \", get_number_processors())\n",
    "print(\"GPU: \", get_gpu_name())\n",
    "print(\"GPU memory: \", get_gpu_memory())\n",
    "print(\"CUDA: \", get_cuda_version())\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset: Netflix\n",
    "\n",
    "This dataset was constructed to support participants in the [Netflix Prize](http://www.netflixprize.com). The movie rating files contain over 100 million ratings from 480 thousand randomly-chosen, anonymous Netflix customers over 17 thousand movie titles.  The data were collected between October, 1998 and December, 2005 and reflect the distribution of all ratings received during this period.  The ratings are on a scale from 1 to 5 (integral) stars.\n",
    "\n",
    "The dataset can be [downloaded here](http://academictorrents.com/details/9b13183dc4d60676b773c9e2cd6de5e5542cee9a). To uncompress it:\n",
    "\n",
    "```bash\n",
    "tar -xvf nf_prize_dataset.tar.gz\n",
    "tar -xf download/training_set.tar\n",
    "```\n",
    "\n",
    "When we download the data, there are two important files:\n",
    "\n",
    "1) The file `training_set.tar` is a tar of a directory containing 17770 files, one per movie.  The first line of each file contains the movie id followed by a colon.  Each subsequent line in the file corresponds to a rating from a customer and its date in the following format:\n",
    "\n",
    "CustomerID,Rating,Date\n",
    "- MovieIDs range from 1 to 17770 sequentially.\n",
    "- CustomerIDs range from 1 to 2649429, with gaps. There are 480189 users.\n",
    "- Ratings are on a five star (integral) scale from 1 to 5.\n",
    "- Dates have the format YYYY-MM-DD.\n",
    "\n",
    "2) Movie information in `movie_titles.txt` is in the following format:\n",
    "\n",
    "MovieID,YearOfRelease,Title\n",
    "\n",
    "- MovieID do not correspond to actual Netflix movie ids or IMDB movie ids.\n",
    "- YearOfRelease can range from 1890 to 2005 and may correspond to the release of corresponding DVD, not necessarily its theaterical release.\n",
    "- Title in English is the Netflix movie.\n",
    "\n",
    "### Data prep\n",
    "\n",
    "The first step is to covert the data to the correct format for the autoencoder to read. This can take between 1 to 2 hours.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%run ./DeepRecommender/data_utils/netflix_data_convert.py $NF_PRIZE_DATASET $NF_DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script splitted the data into train, test and validation set, creating files with three columns: `CustomerID,MovieID,Rating`. The data is splitted over time generating 4 datasets: Netflix 3months, Netflix 6 months, Netflix 1 year and Netflix full. Here there is a table with some details of each dataset:\n",
    "\n",
    "| Dataset  | Netflix 3 months | Netflix 6 months | Netflix 1 year | Netflix full |\n",
    "| -------- | ---------------- | ---------------- | ----------- |  ------------ |\n",
    "| Ratings train | 13,675,402 | 29,179,009 | 41,451,832 | 98,074,901 |\n",
    "| Users train | 311,315 |390,795  | 345,855 | 477,412 |\n",
    "| Items train | 17,736 |17,757  | 16,907 | 17,768 |\n",
    "| Time range train | 2005-09-01 to 2005-11-31 | 2005-06-01 to 2005-11-31 | 2004-06-01 to 2005-05-31 | 1999-12-01 to 2005-11-31\n",
    "|  |  |  |   | |\n",
    "| Ratings test | 2,082,559 | 2,175,535  | 3,888,684| 2,250,481 |\n",
    "| Users test | 160,906 | 169,541  | 197,951| 173,482 |\n",
    "| Items test | 17,261 | 17,290  | 16,506| 17,305 |\n",
    "| Time range test | 2005-12-01 to 2005-12-31 | 2005-12-01 to 2005-12-31 | 2005-06-01 to 2005-06-31 | 2005-12-01 to 2005-12-31\n",
    "\n",
    "Let's take a look at one of the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1041739, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>MovieID</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1549</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>5144</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>7716</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>8348</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4635</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CustomerID  MovieID  Rating\n",
       "0           0     1549     1.0\n",
       "1           0     5144     2.0\n",
       "2           0     7716     3.0\n",
       "3           0     8348     3.0\n",
       "4           0     4635     2.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nf_3m_valid = os.path.join(NF_DATA, 'N3M_VALID', 'n3m.valid.txt')\n",
    "df = pd.read_csv(nf_3m_valid, names=['CustomerID','MovieID','Rating'], sep='\\t')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1040820, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>MovieID</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>159</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>4830</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1261</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>12058</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>13412</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CustomerID  MovieID  Rating\n",
       "0           0      159     4.0\n",
       "1           0     4830     1.0\n",
       "2           0     1261     3.0\n",
       "3           0    12058     3.0\n",
       "4           0    13412     2.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nf_3m_test = os.path.join(NF_DATA, 'N3M_TEST', 'n3m.test.txt')\n",
    "df2 = pd.read_csv(nf_3m_test, names=['CustomerID','MovieID','Rating'], sep='\\t')\n",
    "print(df2.shape)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Autoencoder for Collaborative Filtering\n",
    "\n",
    "Once we have the data, let's explain in some detail the model that we are going to use. The [model](https://arxiv.org/abs/1708.01715) developed by NVIDIA folks is a Deep autoencoder with 6 layers with non-linear activation function SELU (scaled exponential linear units), dropout and iterative dense refeeding.\n",
    "\n",
    "An autoencoder is a network which implements two transformations: $encode(x) : R^n → R^d$ and $decoder(z) : R^d → R^n$. The “goal” of autoenoder is to obtain a $d$ dimensional representation of data such that an error measure between $x$ and $f(x) = decode(encode(x))$ is minimized. In the next figure, the autocoder architecture proposed in the [paper](https://arxiv.org/abs/1708.01715) is showed. Encoder has 2 layers $e_1$ and $e_2$ and decoder has 2 layers $d_1$ and $d_2$. Dropout may be applied to coding layer $z$. In the paper, the authors show experiments with different number of layers, from 2 to 12 (see Table 2 in the original paper).\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"./data/AutoEncoder.png\" width=350px/>\n",
    "</p>\n",
    "\n",
    "During the forward pass the model takes a user representation by his vector of ratings from the training set $x \\in R^n$, where $n$ is number of items. Note that $x$ is very sparse, while the output of the decoder, $y=f(x) \\in R^n$ is dense and contains the rating predictions for all items in the corpus. The loss is the root mean squared error (RMSE).\n",
    "\n",
    "One of the key ideas of the paper is dense re-feeding. Let's consider an idealized scenario with a perfect $f$. Then $f(x)_i = x_i ,∀i : x_i \\ne 0$ and $f(x)_i$ accurately predicts all user's future ratings. This means that if a user rates a new item $k$ (thereby creating a new vector $x'$) then $f(x)_k = x'_k$ and $f(x) = f(x')$. Therefore, the authors refeed the input in the autoencoder to augment the dataset. The method consists of the following steps:\n",
    "\n",
    "1. Given a sparse $x$, compute the forward pass to get $f(x)$ and the loss.\n",
    "\n",
    "2. Backpropagate the loss and update the weights.\n",
    "\n",
    "3. Treat $f(x)$ as a new example and compute $f(f(x))$\n",
    "\n",
    "4. Compute a second backward pass.\n",
    "\n",
    "Steps 3 and 4 can be repeated several times.\n",
    "\n",
    "Finally, the authors explore different non-linear [activation functions](https://github.com/pytorch/pytorch/blob/master/torch/nn/functional.py). They found that on this task ELU, SELU and LRELU, which have non-zero negative parts, perform much better than SIGMOID, RELU, RELU6, and TANH."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(aug_step=1, batch_size=128, constrained=False, drop_prob=0.8, gpu_ids='0', hidden_layers='512,512,1024', logdir='model_save', lr=0.005, noise_prob=0.0, non_linearity_type='selu', num_epochs=10, optimizer='momentum', path_to_eval_data='Netflix/N3M_VALID', path_to_train_data='Netflix/N3M_TRAIN', skip_last_layer_nl=False, weight_decay=0.0)\n",
      "Loading training data from Netflix/N3M_TRAIN\n",
      "Data loaded\n",
      "Total items found: 311315\n",
      "Vector dim: 17736\n",
      "Loading eval data from Netflix/N3M_VALID\n",
      "******************************\n",
      "******************************\n",
      "[17736, 512, 512, 1024]\n",
      "Dropout drop probability: 0.8\n",
      "Encoder pass:\n",
      "torch.Size([512, 17736])\n",
      "torch.Size([512])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([512])\n",
      "torch.Size([1024, 512])\n",
      "torch.Size([1024])\n",
      "Decoder pass:\n",
      "torch.Size([512, 1024])\n",
      "torch.Size([512])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([512])\n",
      "torch.Size([17736, 512])\n",
      "torch.Size([17736])\n",
      "******************************\n",
      "******************************\n",
      "Using GPUs: [0]\n",
      "Doing epoch 0 of 10\n",
      "Total epoch 0 finished in 69.39120125770569 seconds with TRAINING RMSE loss: 1.1183533288603893\n",
      "Epoch 0 EVALUATION LOSS: 0.997187149064757\n",
      "Doing epoch 1 of 10\n",
      "Total epoch 1 finished in 69.05308318138123 seconds with TRAINING RMSE loss: 0.9789836858425376\n",
      "Epoch 1 EVALUATION LOSS: 0.9830844731127444\n",
      "Doing epoch 2 of 10\n",
      "Total epoch 2 finished in 68.9950942993164 seconds with TRAINING RMSE loss: 0.9593065493420863\n",
      "Epoch 2 EVALUATION LOSS: 0.98861261075587\n",
      "Doing epoch 3 of 10\n",
      "Total epoch 3 finished in 69.13668775558472 seconds with TRAINING RMSE loss: 0.9464339257342224\n",
      "Epoch 3 EVALUATION LOSS: 0.9762008394628371\n",
      "Doing epoch 4 of 10\n",
      "Total epoch 4 finished in 69.09636783599854 seconds with TRAINING RMSE loss: 0.9355610656442712\n",
      "Epoch 4 EVALUATION LOSS: 0.9808872814542353\n",
      "Doing epoch 5 of 10\n",
      "Total epoch 5 finished in 69.17095017433167 seconds with TRAINING RMSE loss: 0.9258845933913559\n",
      "Epoch 5 EVALUATION LOSS: 0.9842890565637975\n",
      "Doing epoch 6 of 10\n",
      "Total epoch 6 finished in 69.08278322219849 seconds with TRAINING RMSE loss: 0.9622985019130311\n",
      "Epoch 6 EVALUATION LOSS: 0.9838701840431763\n",
      "Doing epoch 7 of 10\n",
      "Total epoch 7 finished in 69.04743361473083 seconds with TRAINING RMSE loss: 0.9337027765558531\n",
      "Epoch 7 EVALUATION LOSS: 0.984365107262594\n",
      "Doing epoch 8 of 10\n",
      "Total epoch 8 finished in 69.1117844581604 seconds with TRAINING RMSE loss: 0.9205498934527924\n",
      "Epoch 8 EVALUATION LOSS: 0.9750949946703523\n",
      "Doing epoch 9 of 10\n",
      "Total epoch 9 finished in 69.13744783401489 seconds with TRAINING RMSE loss: 0.9099730840527471\n",
      "Epoch 9 EVALUATION LOSS: 0.9742177051690417\n",
      "Saving model to model_save/model.epoch_9\n",
      "Routine finished. Process time 2747.513389825821 s\n"
     ]
    }
   ],
   "source": [
    "%run ./DeepRecommender/run.py --gpu_ids $GPUS \\\n",
    "    --path_to_train_data $TRAIN \\\n",
    "    --path_to_eval_data $EVAL \\\n",
    "    --hidden_layers $HIDDEN \\\n",
    "    --non_linearity_type $ACTIVATION \\\n",
    "    --batch_size $BATCH_SIZE \\\n",
    "    --logdir $MODEL_OUTPUT_DIR \\\n",
    "    --drop_prob $DROPOUT \\\n",
    "    --optimizer $OPTIMIZER \\\n",
    "    --lr $LR \\\n",
    "    --weight_decay $WD \\\n",
    "    --aug_step $AUG_STEP \\\n",
    "    --num_epochs $EPOCHS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "Now we are going to evaluate the model on the test set and compute the final loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(constrained=False, drop_prob=0.8, hidden_layers='512,512,1024', non_linearity_type='selu', path_to_eval_data='Netflix/N3M_TEST', path_to_train_data='Netflix/N3M_TRAIN', predictions_path='preds.txt', save_path='model_save/model.epoch_9', skip_last_layer_nl=False)\n",
      "Loading training data\n",
      "Data loaded\n",
      "Total items found: 311315\n",
      "Vector dim: 17736\n",
      "Loading eval data\n",
      "******************************\n",
      "******************************\n",
      "[17736, 512, 512, 1024]\n",
      "Dropout drop probability: 0.8\n",
      "Encoder pass:\n",
      "torch.Size([512, 17736])\n",
      "torch.Size([512])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([512])\n",
      "torch.Size([1024, 512])\n",
      "torch.Size([1024])\n",
      "Decoder pass:\n",
      "torch.Size([512, 1024])\n",
      "torch.Size([512])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([512])\n",
      "torch.Size([17736, 512])\n",
      "torch.Size([17736])\n",
      "******************************\n",
      "******************************\n",
      "Loading model from: model_save/model.epoch_9\n",
      "Done: 0\n",
      "Done: 10000\n",
      "Done: 20000\n",
      "Done: 30000\n",
      "Done: 40000\n",
      "Done: 50000\n",
      "Done: 60000\n",
      "Done: 70000\n",
      "Done: 80000\n",
      "Done: 90000\n",
      "Done: 100000\n",
      "Done: 110000\n",
      "Done: 120000\n",
      "Routine finished. Process time 228.1412889957428 s\n"
     ]
    }
   ],
   "source": [
    "%run ./DeepRecommender/infer.py \\\n",
    "--path_to_train_data $TRAIN \\\n",
    "--path_to_eval_data $TEST \\\n",
    "--hidden_layers $HIDDEN \\\n",
    "--non_linearity_type $ACTIVATION \\\n",
    "--save_path  $MODEL_PATH \\\n",
    "--drop_prob $DROPOUT \\\n",
    "--predictions_path $INFER_OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(path_to_predictions='preds.txt', round=False)\n",
      "####################\n",
      "RMSE: 0.9746437597050387\n",
      "####################\n"
     ]
    }
   ],
   "source": [
    "%run ./DeepRecommender/compute_RMSE.py --path_to_predictions=$INFER_OUTPUT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do something more real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MovieID</th>\n",
       "      <th>Year</th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>Dinosaur Planet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2004.0</td>\n",
       "      <td>Isle of Man TT 2004 Review</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1997.0</td>\n",
       "      <td>Character</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1994.0</td>\n",
       "      <td>Paula Abdul's Get Up &amp; Dance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2004.0</td>\n",
       "      <td>The Rise and Fall of ECW</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MovieID    Year                         Title\n",
       "0        1  2003.0               Dinosaur Planet\n",
       "1        2  2004.0    Isle of Man TT 2004 Review\n",
       "2        3  1997.0                     Character\n",
       "3        4  1994.0  Paula Abdul's Get Up & Dance\n",
       "4        5  2004.0      The Rise and Fall of ECW"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles = pd.read_csv(MOVIE_TITLES, names=['MovieID','Year','Title'], encoding = \"latin\")\n",
    "titles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "me_gusta = [13, 191, 209, 316, 345, 468, 560, 752, 1066, 1551, 1601, 1905, 2189, 2252, 5507] \n",
    "me_gusta_pred = [2452, 2532, 2689, 3012, 3287]\n",
    "booo = [148, 270, 571, 559, 1195, 1400, 1387, 1947, 1962,  2282, 2457, 2803, 3106, 3414, 4308]\n",
    "booo_pred = [4783, 4894, 5088, 5107, 5184]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MovieID</th>\n",
       "      <th>Year</th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>Lord of the Rings: The Return of the King: Ext...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>191</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>X2: X-Men United</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>209</td>\n",
       "      <td>1996.0</td>\n",
       "      <td>Star Trek: Deep Space Nine: Season 5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>316</td>\n",
       "      <td>1999.0</td>\n",
       "      <td>Futurama: Monster Robot Maniac Fun Collection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>345</td>\n",
       "      <td>1998.0</td>\n",
       "      <td>Star Trek: Voyager: Season 5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>468</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>The Matrix: Revolutions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>560</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>Star Trek: Enterprise: Season 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751</th>\n",
       "      <td>752</td>\n",
       "      <td>1993.0</td>\n",
       "      <td>Star Trek: The Next Generation: Season 7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1065</th>\n",
       "      <td>1066</td>\n",
       "      <td>1978.0</td>\n",
       "      <td>Superman: The Movie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1550</th>\n",
       "      <td>1551</td>\n",
       "      <td>2004.0</td>\n",
       "      <td>Spider-Man 2: Bonus Material</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1600</th>\n",
       "      <td>1601</td>\n",
       "      <td>2004.0</td>\n",
       "      <td>J.R.R. Tolkien and the Birth of The Lord of th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1904</th>\n",
       "      <td>1905</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>Pirates of the Caribbean: The Curse of the Bla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2188</th>\n",
       "      <td>2189</td>\n",
       "      <td>1981.0</td>\n",
       "      <td>The Hitchhiker's Guide to the Galaxy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2251</th>\n",
       "      <td>2252</td>\n",
       "      <td>1992.0</td>\n",
       "      <td>Bram Stoker's Dracula</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5506</th>\n",
       "      <td>5507</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>Dragon Ball: Fortune Teller Baba Saga</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      MovieID    Year                                              Title\n",
       "12         13  2003.0  Lord of the Rings: The Return of the King: Ext...\n",
       "190       191  2003.0                                   X2: X-Men United\n",
       "208       209  1996.0               Star Trek: Deep Space Nine: Season 5\n",
       "315       316  1999.0      Futurama: Monster Robot Maniac Fun Collection\n",
       "344       345  1998.0                       Star Trek: Voyager: Season 5\n",
       "467       468  2003.0                            The Matrix: Revolutions\n",
       "559       560  2003.0                    Star Trek: Enterprise: Season 3\n",
       "751       752  1993.0           Star Trek: The Next Generation: Season 7\n",
       "1065     1066  1978.0                                Superman: The Movie\n",
       "1550     1551  2004.0                       Spider-Man 2: Bonus Material\n",
       "1600     1601  2004.0  J.R.R. Tolkien and the Birth of The Lord of th...\n",
       "1904     1905  2003.0  Pirates of the Caribbean: The Curse of the Bla...\n",
       "2188     2189  1981.0               The Hitchhiker's Guide to the Galaxy\n",
       "2251     2252  1992.0                              Bram Stoker's Dracula\n",
       "5506     5507  2002.0              Dragon Ball: Fortune Teller Baba Saga"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_me_gusta = titles[titles['MovieID'].isin(me_gusta)]\n",
    "df_me_gusta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MovieID</th>\n",
       "      <th>Year</th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2451</th>\n",
       "      <td>2452</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>Lord of the Rings: The Fellowship of the Ring</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2531</th>\n",
       "      <td>2532</td>\n",
       "      <td>1999.0</td>\n",
       "      <td>Futurama: Vol. 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2688</th>\n",
       "      <td>2689</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>Minority Report: Bonus Material</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3011</th>\n",
       "      <td>3012</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>Dragon Ball Z: World Tournament</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3286</th>\n",
       "      <td>3287</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>Terminator 3: Rise of the Machines: Bonus Mate...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      MovieID    Year                                              Title\n",
       "2451     2452  2001.0      Lord of the Rings: The Fellowship of the Ring\n",
       "2531     2532  1999.0                                   Futurama: Vol. 1\n",
       "2688     2689  2002.0                    Minority Report: Bonus Material\n",
       "3011     3012  2001.0                    Dragon Ball Z: World Tournament\n",
       "3286     3287  2003.0  Terminator 3: Rise of the Machines: Bonus Mate..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_me_gusta_pred = titles[titles['MovieID'].isin(me_gusta_pred)]\n",
    "df_me_gusta_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MovieID</th>\n",
       "      <th>Year</th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>148</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>Sweet November</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>270</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>Sex and the City: Season 4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>559</td>\n",
       "      <td>1940.0</td>\n",
       "      <td>Rebecca: Bonus Material</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570</th>\n",
       "      <td>571</td>\n",
       "      <td>1999.0</td>\n",
       "      <td>American Beauty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1194</th>\n",
       "      <td>1195</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>Madonna: The Girlie Show: Live Down Under</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1386</th>\n",
       "      <td>1387</td>\n",
       "      <td>1999.0</td>\n",
       "      <td>The Girl Next Door</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1399</th>\n",
       "      <td>1400</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>Britney Spears: Britney in Hawaii: Live and More</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1946</th>\n",
       "      <td>1947</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>Gilmore Girls: Season 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1961</th>\n",
       "      <td>1962</td>\n",
       "      <td>2004.0</td>\n",
       "      <td>50 First Dates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2281</th>\n",
       "      <td>2282</td>\n",
       "      <td>2005.0</td>\n",
       "      <td>Disney Princess Stories: Vol. 2: Tales of Frie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2456</th>\n",
       "      <td>2457</td>\n",
       "      <td>2004.0</td>\n",
       "      <td>A Cinderella Story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2802</th>\n",
       "      <td>2803</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>Pride and Prejudice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3105</th>\n",
       "      <td>3106</td>\n",
       "      <td>1990.0</td>\n",
       "      <td>Ghost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3413</th>\n",
       "      <td>3414</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>Pocahontas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4307</th>\n",
       "      <td>4308</td>\n",
       "      <td>1999.0</td>\n",
       "      <td>Beauty and the Beast</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      MovieID    Year                                              Title\n",
       "147       148  2001.0                                     Sweet November\n",
       "269       270  2001.0                         Sex and the City: Season 4\n",
       "558       559  1940.0                            Rebecca: Bonus Material\n",
       "570       571  1999.0                                    American Beauty\n",
       "1194     1195  1988.0          Madonna: The Girlie Show: Live Down Under\n",
       "1386     1387  1999.0                                 The Girl Next Door\n",
       "1399     1400  2000.0   Britney Spears: Britney in Hawaii: Live and More\n",
       "1946     1947  2002.0                            Gilmore Girls: Season 3\n",
       "1961     1962  2004.0                                     50 First Dates\n",
       "2281     2282  2005.0  Disney Princess Stories: Vol. 2: Tales of Frie...\n",
       "2456     2457  2004.0                                 A Cinderella Story\n",
       "2802     2803  1995.0                                Pride and Prejudice\n",
       "3105     3106  1990.0                                              Ghost\n",
       "3413     3414  1995.0                                         Pocahontas\n",
       "4307     4308  1999.0                               Beauty and the Beast"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_booo = titles[titles['MovieID'].isin(booo)]\n",
    "df_booo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MovieID</th>\n",
       "      <th>Year</th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4782</th>\n",
       "      <td>4783</td>\n",
       "      <td>1999.0</td>\n",
       "      <td>Felicity: Season 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4893</th>\n",
       "      <td>4894</td>\n",
       "      <td>1998.0</td>\n",
       "      <td>Celia Cruz: Fania Allstars in Africa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5087</th>\n",
       "      <td>5088</td>\n",
       "      <td>1987.0</td>\n",
       "      <td>Dirty Dancing: Bonus Material</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5106</th>\n",
       "      <td>5107</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>Barbra Streisand: Timeless: Live in Concert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5183</th>\n",
       "      <td>5184</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>'N Sync: Making of the Tour</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      MovieID    Year                                        Title\n",
       "4782     4783  1999.0                           Felicity: Season 2\n",
       "4893     4894  1998.0         Celia Cruz: Fania Allstars in Africa\n",
       "5087     5088  1987.0                Dirty Dancing: Bonus Material\n",
       "5106     5107  2001.0  Barbra Streisand: Timeless: Live in Concert\n",
       "5183     5184  2000.0                  'N Sync: Making of the Tour"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_booo_pred = titles[titles['MovieID'].isin(booo_pred)]\n",
    "df_booo_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_MOVIES = './user/miguel.txt'\n",
    "USER_PRED = 'miguel_pred.txt'\n",
    "CUSTOMER_ID = 0 #500000-> gets key error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user = pd.DataFrame({'CustomerID':CUSTOMER_ID, 'MovieId':df_me_gusta['MovieID'], 'Ratings':5.0})\n",
    "df_user = pd.concat([df_user, pd.DataFrame({'CustomerID':CUSTOMER_ID, 'MovieId':df_booo['MovieID'], 'Ratings':1.0})])\n",
    "df_user_pred = pd.DataFrame({'CustomerID':CUSTOMER_ID, 'MovieId':df_me_gusta_pred['MovieID'], 'Ratings':5.0})\n",
    "df_user_pred = pd.concat([df_user_pred, pd.DataFrame({'CustomerID':CUSTOMER_ID, 'MovieId':df_booo_pred['MovieID'], 'Ratings':1.0})])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user.to_csv(USER_MOVIES, sep='\\t',header=False, index=False)\n",
    "df_user_pred.to_csv(USER_PRED, sep='\\t',header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(constrained=False, drop_prob=0.8, hidden_layers='512,512,1024', non_linearity_type='selu', path_to_eval_data='./Netflix/test1', path_to_train_data='Netflix/N3M_TRAIN', predictions_path='pred2.txt', save_path='model_save/model.epoch_9', skip_last_layer_nl=False)\n",
      "Loading training data\n",
      "Data loaded\n",
      "Total items found: 311315\n",
      "Vector dim: 17736\n",
      "Loading eval data\n",
      "******************************\n",
      "******************************\n",
      "[17736, 512, 512, 1024]\n",
      "Dropout drop probability: 0.8\n",
      "Encoder pass:\n",
      "torch.Size([512, 17736])\n",
      "torch.Size([512])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([512])\n",
      "torch.Size([1024, 512])\n",
      "torch.Size([1024])\n",
      "Decoder pass:\n",
      "torch.Size([512, 1024])\n",
      "torch.Size([512])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([512])\n",
      "torch.Size([17736, 512])\n",
      "torch.Size([17736])\n",
      "******************************\n",
      "******************************\n",
      "Loading model from: model_save/model.epoch_9\n",
      "Done: 0\n",
      "Routine finished. Process time 54.71354603767395 s\n"
     ]
    }
   ],
   "source": [
    "%run ./DeepRecommender/infer.py \\\n",
    "--path_to_train_data $TRAIN \\\n",
    "--path_to_eval_data ./Netflix/test1 \\\n",
    "--hidden_layers $HIDDEN \\\n",
    "--non_linearity_type $ACTIVATION \\\n",
    "--save_path  $MODEL_PATH \\\n",
    "--drop_prob $DROPOUT \\\n",
    "--predictions_path pred2.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(constrained=False, drop_prob=0.8, hidden_layers='512,512,1024', non_linearity_type='selu', path_to_eval_data='./Netflix/test2', path_to_train_data='Netflix/N3M_TRAIN', predictions_path='pred_test2.txt', save_path='model_save/model.epoch_9', skip_last_layer_nl=False)\n",
      "Loading training data\n",
      "Data loaded\n",
      "Total items found: 311315\n",
      "Vector dim: 17736\n",
      "Loading eval data\n",
      "******************************\n",
      "******************************\n",
      "[17736, 512, 512, 1024]\n",
      "Dropout drop probability: 0.8\n",
      "Encoder pass:\n",
      "torch.Size([512, 17736])\n",
      "torch.Size([512])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([512])\n",
      "torch.Size([1024, 512])\n",
      "torch.Size([1024])\n",
      "Decoder pass:\n",
      "torch.Size([512, 1024])\n",
      "torch.Size([512])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([512])\n",
      "torch.Size([17736, 512])\n",
      "torch.Size([17736])\n",
      "******************************\n",
      "******************************\n",
      "Loading model from: model_save/model.epoch_9\n",
      "Done: 0\n",
      "Routine finished. Process time 55.23368763923645 s\n"
     ]
    }
   ],
   "source": [
    "%run ./DeepRecommender/infer.py \\\n",
    "--path_to_train_data $TRAIN \\\n",
    "--path_to_eval_data ./Netflix/test2 \\\n",
    "--hidden_layers $HIDDEN \\\n",
    "--non_linearity_type $ACTIVATION \\\n",
    "--save_path  $MODEL_PATH \\\n",
    "--drop_prob $DROPOUT \\\n",
    "--predictions_path pred_test2.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(constrained=False, drop_prob=0.8, hidden_layers='512,512,1024', non_linearity_type='selu', path_to_eval_data='./user_short', path_to_train_data='Netflix/N3M_TRAIN', predictions_path='pred_user3.txt', save_path='model_save/model.epoch_9', skip_last_layer_nl=False)\n",
      "Loading training data\n",
      "Data loaded\n",
      "Total items found: 311315\n",
      "Vector dim: 17736\n",
      "Loading eval data\n",
      "******************************\n",
      "******************************\n",
      "[17736, 512, 512, 1024]\n",
      "Dropout drop probability: 0.8\n",
      "Encoder pass:\n",
      "torch.Size([512, 17736])\n",
      "torch.Size([512])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([512])\n",
      "torch.Size([1024, 512])\n",
      "torch.Size([1024])\n",
      "Decoder pass:\n",
      "torch.Size([512, 1024])\n",
      "torch.Size([512])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([512])\n",
      "torch.Size([17736, 512])\n",
      "torch.Size([17736])\n",
      "******************************\n",
      "******************************\n",
      "Loading model from: model_save/model.epoch_9\n",
      "len= 17736\n",
      "non_zeros  [4766, 9777, 15311]\n",
      "13\n",
      "191\n",
      "209\n",
      "Done: 0\n",
      "Routine finished. Process time 57.151479721069336 s\n"
     ]
    }
   ],
   "source": [
    "%run ./DeepRecommender/infer.py \\\n",
    "--path_to_train_data $TRAIN \\\n",
    "--path_to_eval_data ./user_short \\\n",
    "--hidden_layers $HIDDEN \\\n",
    "--non_linearity_type $ACTIVATION \\\n",
    "--save_path  $MODEL_PATH \\\n",
    "--drop_prob $DROPOUT \\\n",
    "--predictions_path pred_user3.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data\n"
     ]
    }
   ],
   "source": [
    "  from DeepRecommender.reco_encoder.data import input_layer\n",
    "  params = dict()\n",
    "  params['batch_size'] = 1\n",
    "  params['data_dir'] =  TRAIN\n",
    "  params['major'] = 'users'\n",
    "  params['itemIdInd'] = 1\n",
    "  params['userIdInd'] = 0\n",
    "  print(\"Loading training data\")\n",
    "  data_layer = input_layer.UserItemRecDataProvider(params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17736\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'batch_size': 1,\n",
       " 'data_dir': 'Netflix/N3M_TRAIN',\n",
       " 'itemIdInd': 1,\n",
       " 'major': 'users',\n",
       " 'userIdInd': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data_layer.vector_dim)\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "  import copy\n",
    "  eval_params = copy.deepcopy(params)\n",
    "  # must set eval batch size to 1 to make sure no examples are missed\n",
    "  eval_params['batch_size'] = 1\n",
    "  eval_params['data_dir'] = TEST\n",
    "  eval_data_layer = input_layer.UserItemRecDataProvider(params=eval_params,\n",
    "                                                        user_id_map=data_layer.userIdMap,\n",
    "                                                        item_id_map=data_layer.itemIdMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17736\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'batch_size': 1,\n",
       " 'data_dir': 'Netflix/N3M_TEST',\n",
       " 'itemIdInd': 1,\n",
       " 'major': 'users',\n",
       " 'userIdInd': 0}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(eval_data_layer.vector_dim)\n",
    "eval_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16243\n"
     ]
    }
   ],
   "source": [
    "len(list(set(eval_data_layer.data.keys())))\n",
    "ll = set()\n",
    "for k,i in eval_data_layer.data.items():\n",
    "    for movie, rating in i:\n",
    "        ll.add(movie)\n",
    "    \n",
    "print(len(list(ll)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{13: 5.0,\n",
       " 148: 1.0,\n",
       " 191: 5.0,\n",
       " 209: 5.0,\n",
       " 270: 1.0,\n",
       " 316: 5.0,\n",
       " 345: 5.0,\n",
       " 468: 5.0,\n",
       " 559: 1.0,\n",
       " 560: 5.0,\n",
       " 571: 1.0,\n",
       " 752: 5.0,\n",
       " 1066: 5.0,\n",
       " 1195: 1.0,\n",
       " 1387: 1.0,\n",
       " 1400: 1.0,\n",
       " 1551: 5.0,\n",
       " 1601: 5.0,\n",
       " 1905: 5.0,\n",
       " 1947: 1.0,\n",
       " 1962: 1.0,\n",
       " 2189: 5.0,\n",
       " 2252: 5.0,\n",
       " 2282: 1.0,\n",
       " 2457: 1.0,\n",
       " 2803: 1.0,\n",
       " 3106: 1.0,\n",
       " 3414: 1.0,\n",
       " 4308: 1.0,\n",
       " 5507: 5.0}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_query = df_user.drop(['CustomerID'], axis=1).set_index('MovieId')\n",
    "dict_query = df_query.to_dict()['Ratings']\n",
    "dict_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "  from DeepRecommender.reco_encoder.data import input_layer_api\n",
    "  params_api = dict()\n",
    "  params_api['batch_size'] = 1\n",
    "  params_api['data_dict'] =  dict_query\n",
    "  params_api['major'] = 'users'\n",
    "  params_api['itemIdInd'] = 1\n",
    "  params_api['userIdInd'] = 0\n",
    "  data_api = input_layer_api.UserItemRecDataProviderAPI(params=params_api,\n",
    "                                                        user_id_map=data_layer.userIdMap,\n",
    "                                                        item_id_map=data_layer.itemIdMap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(1601, 5.0),\n",
       "  (5507, 5.0),\n",
       "  (2252, 5.0),\n",
       "  (13, 5.0),\n",
       "  (270, 1.0),\n",
       "  (1551, 5.0),\n",
       "  (2189, 5.0),\n",
       "  (209, 5.0),\n",
       "  (148, 1.0),\n",
       "  (3414, 1.0),\n",
       "  (2457, 1.0),\n",
       "  (345, 5.0),\n",
       "  (1947, 1.0),\n",
       "  (2282, 1.0),\n",
       "  (560, 5.0),\n",
       "  (3106, 1.0),\n",
       "  (4308, 1.0),\n",
       "  (1066, 5.0),\n",
       "  (1387, 1.0),\n",
       "  (559, 1.0),\n",
       "  (752, 5.0),\n",
       "  (1905, 5.0),\n",
       "  (2803, 1.0),\n",
       "  (1195, 1.0),\n",
       "  (1400, 1.0),\n",
       "  (468, 5.0),\n",
       "  (571, 1.0),\n",
       "  (316, 5.0),\n",
       "  (1962, 1.0),\n",
       "  (191, 5.0)]}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_api.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17736"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_api.vector_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************\n",
      "******************************\n",
      "[17736, 512, 512, 1024]\n",
      "Dropout drop probability: 0.8\n",
      "Encoder pass:\n",
      "torch.Size([512, 17736])\n",
      "torch.Size([512])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([512])\n",
      "torch.Size([1024, 512])\n",
      "torch.Size([1024])\n",
      "Decoder pass:\n",
      "torch.Size([512, 1024])\n",
      "torch.Size([512])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([512])\n",
      "torch.Size([17736, 512])\n",
      "torch.Size([17736])\n",
      "******************************\n",
      "******************************\n"
     ]
    }
   ],
   "source": [
    "from DeepRecommender.reco_encoder.model import model\n",
    "rencoder_api = model.AutoEncoder(layer_sizes=[data_layer.vector_dim] + [int(l) for l in HIDDEN.split(',')],\n",
    "                               nl_type=ACTIVATION,\n",
    "                               is_constrained=False,\n",
    "                               dp_drop_prob=DROPOUT,\n",
    "                               last_layer_activations=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: ./model_save/model.epoch_9\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "def load_model_weights(model_architecture, weights_path):\n",
    "  if os.path.isfile(weights_path):\n",
    "    print(\"Loading model from: {}\".format(weights_path))\n",
    "    model_architecture.load_state_dict(torch.load(weights_path))\n",
    "  else:\n",
    "    raise ValueError(\"Path not found {}\".format(weights_path))\n",
    "\n",
    "load_model_weights(rencoder_api, './model_save/model.epoch_9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('encode_w.0', \n",
       "               1.3762e-02 -6.6482e-03  1.9167e-02  ...   4.8230e-03  1.1178e-02 -7.4413e-03\n",
       "              -1.2196e-02 -2.8160e-03 -4.9494e-03  ...   1.3009e-02 -5.3862e-03  1.5944e-02\n",
       "               6.3024e-03 -1.7319e-02  5.8461e-03  ...   7.4736e-04 -1.6535e-02 -9.1369e-03\n",
       "                              ...                   ⋱                   ...                \n",
       "               2.0446e-02  3.2055e-03 -3.0206e-02  ...   1.9669e-03 -6.5677e-03 -9.1297e-03\n",
       "              -4.1579e-02  1.0100e-02 -2.2402e-03  ...   1.7774e-02  5.5699e-04  1.0243e-02\n",
       "               8.0284e-03 -1.1262e-02 -2.2738e-02  ...   1.2861e-02  1.4607e-02  7.3368e-03\n",
       "              [torch.FloatTensor of size 512x17736]), ('encode_w.1', \n",
       "               2.5071e-02  4.3264e-02  1.2727e-02  ...   8.6099e-02 -1.3217e-02  4.7283e-02\n",
       "              -1.8742e-02 -1.6810e-02  6.8249e-02  ...   3.6553e-03  2.3814e-02 -2.4263e-02\n",
       "               7.1702e-02  3.5000e-02  1.1755e-02  ...   7.2973e-02  6.8955e-02 -4.6599e-02\n",
       "                              ...                   ⋱                   ...                \n",
       "               5.0547e-02  5.7333e-02  2.4498e-02  ...   1.6445e-02 -2.3102e-02 -6.4239e-02\n",
       "               3.3259e-02 -5.2140e-02  1.8264e-02  ...  -1.2082e-03 -8.1967e-04  6.0038e-02\n",
       "              -4.3427e-02 -3.5593e-02  2.8677e-02  ...  -2.2574e-02  5.1597e-02  4.9621e-03\n",
       "              [torch.FloatTensor of size 512x512]), ('encode_w.2', \n",
       "              -9.6269e-03 -2.2572e-02  5.1609e-02  ...   5.2145e-02  3.4375e-02 -6.2506e-02\n",
       "               3.9751e-02  4.5264e-02 -3.4301e-02  ...  -4.4828e-02 -3.9497e-02  5.2647e-02\n",
       "               4.5591e-02  1.4200e-02 -3.9866e-03  ...  -5.4562e-02  2.9469e-02 -5.6910e-02\n",
       "                              ...                   ⋱                   ...                \n",
       "               7.1896e-02 -2.0406e-02  6.5646e-04  ...   4.3976e-02  2.5651e-02 -6.0694e-02\n",
       "               4.8600e-02 -1.9008e-02  2.0461e-02  ...   5.8242e-03  3.2578e-02 -3.1125e-02\n",
       "               4.3656e-05 -4.4468e-03 -4.5361e-02  ...  -4.8945e-02 -2.8077e-02 -2.6705e-02\n",
       "              [torch.FloatTensor of size 1024x512]), ('encode_b.0', \n",
       "              -0.1159\n",
       "              -0.1540\n",
       "              -0.0320\n",
       "              -0.0961\n",
       "               0.0501\n",
       "               0.0283\n",
       "               0.0210\n",
       "               0.0292\n",
       "              -0.0160\n",
       "               0.0453\n",
       "              -0.0138\n",
       "              -0.0133\n",
       "              -0.0618\n",
       "              -0.0081\n",
       "              -0.1115\n",
       "              -0.0086\n",
       "              -0.0816\n",
       "              -0.0464\n",
       "               0.0913\n",
       "              -0.1025\n",
       "              -0.0456\n",
       "              -0.1186\n",
       "               0.0103\n",
       "               0.0262\n",
       "              -0.0701\n",
       "              -0.0434\n",
       "               0.0300\n",
       "              -0.0478\n",
       "              -0.0119\n",
       "              -0.0245\n",
       "              -0.0826\n",
       "               0.0406\n",
       "              -0.0494\n",
       "              -0.0627\n",
       "               0.0609\n",
       "              -0.0983\n",
       "               0.0928\n",
       "               0.0010\n",
       "              -0.0433\n",
       "              -0.0139\n",
       "               0.0227\n",
       "               0.0080\n",
       "              -0.1367\n",
       "              -0.0101\n",
       "               0.0162\n",
       "              -0.1290\n",
       "              -0.0093\n",
       "               0.0089\n",
       "               0.0135\n",
       "              -0.0868\n",
       "              -0.0784\n",
       "              -0.0707\n",
       "               0.0711\n",
       "              -0.1087\n",
       "              -0.0148\n",
       "              -0.0570\n",
       "               0.0287\n",
       "               0.0188\n",
       "              -0.0568\n",
       "              -0.0121\n",
       "               0.0117\n",
       "              -0.0078\n",
       "               0.0325\n",
       "              -0.1214\n",
       "              -0.0992\n",
       "              -0.0581\n",
       "              -0.0557\n",
       "               0.0599\n",
       "              -0.0810\n",
       "              -0.0533\n",
       "              -0.0596\n",
       "              -0.0158\n",
       "               0.0629\n",
       "               0.0446\n",
       "              -0.0106\n",
       "              -0.0414\n",
       "              -0.0477\n",
       "               0.0468\n",
       "               0.0594\n",
       "              -0.0704\n",
       "              -0.0775\n",
       "              -0.0327\n",
       "               0.0092\n",
       "               0.0606\n",
       "               0.0454\n",
       "              -0.0427\n",
       "              -0.0356\n",
       "               0.0009\n",
       "              -0.1485\n",
       "              -0.1237\n",
       "               0.0114\n",
       "               0.0462\n",
       "               0.0619\n",
       "              -0.0720\n",
       "              -0.0066\n",
       "               0.0024\n",
       "               0.0033\n",
       "              -0.0624\n",
       "              -0.0059\n",
       "              -0.0180\n",
       "              -0.1011\n",
       "               0.0704\n",
       "               0.0798\n",
       "              -0.0932\n",
       "              -0.0311\n",
       "               0.0471\n",
       "              -0.0378\n",
       "              -0.0100\n",
       "               0.0140\n",
       "              -0.0477\n",
       "              -0.0025\n",
       "              -0.0714\n",
       "              -0.0524\n",
       "              -0.0042\n",
       "              -0.0820\n",
       "               0.0458\n",
       "              -0.0607\n",
       "              -0.0384\n",
       "               0.0096\n",
       "               0.0208\n",
       "               0.0389\n",
       "              -0.0854\n",
       "               0.0419\n",
       "              -0.0966\n",
       "              -0.0458\n",
       "              -0.0792\n",
       "               0.0458\n",
       "              -0.0154\n",
       "              -0.0297\n",
       "              -0.1121\n",
       "              -0.0634\n",
       "              -0.0342\n",
       "              -0.0373\n",
       "               0.1220\n",
       "               0.0384\n",
       "              -0.0799\n",
       "              -0.0884\n",
       "               0.0027\n",
       "               0.0002\n",
       "              -0.0029\n",
       "              -0.0664\n",
       "              -0.0651\n",
       "              -0.0796\n",
       "              -0.0219\n",
       "              -0.0056\n",
       "              -0.0418\n",
       "              -0.0830\n",
       "              -0.1209\n",
       "              -0.1654\n",
       "               0.0305\n",
       "               0.0748\n",
       "               0.0162\n",
       "              -0.0741\n",
       "              -0.0071\n",
       "               0.0151\n",
       "              -0.0192\n",
       "               0.0049\n",
       "               0.0041\n",
       "              -0.0255\n",
       "              -0.0697\n",
       "              -0.0450\n",
       "              -0.0288\n",
       "              -0.0316\n",
       "              -0.0941\n",
       "               0.0339\n",
       "               0.0175\n",
       "              -0.0895\n",
       "              -0.0462\n",
       "              -0.0445\n",
       "              -0.0015\n",
       "              -0.0573\n",
       "               0.0289\n",
       "               0.0620\n",
       "              -0.0295\n",
       "               0.0540\n",
       "              -0.0886\n",
       "              -0.1557\n",
       "              -0.0647\n",
       "              -0.1485\n",
       "               0.0215\n",
       "              -0.0646\n",
       "              -0.0269\n",
       "              -0.0049\n",
       "               0.0146\n",
       "               0.0838\n",
       "               0.0604\n",
       "               0.0065\n",
       "               0.0123\n",
       "              -0.0103\n",
       "              -0.0360\n",
       "              -0.0753\n",
       "              -0.0612\n",
       "              -0.0371\n",
       "              -0.0571\n",
       "              -0.0317\n",
       "              -0.1437\n",
       "               0.0125\n",
       "              -0.1228\n",
       "              -0.0963\n",
       "              -0.0315\n",
       "               0.0401\n",
       "              -0.0587\n",
       "              -0.0579\n",
       "              -0.0031\n",
       "              -0.0303\n",
       "              -0.1121\n",
       "               0.0123\n",
       "               0.0129\n",
       "              -0.0734\n",
       "              -0.0492\n",
       "              -0.1012\n",
       "              -0.1128\n",
       "              -0.0308\n",
       "              -0.0200\n",
       "               0.0628\n",
       "              -0.1451\n",
       "              -0.0313\n",
       "              -0.0197\n",
       "               0.0246\n",
       "               0.0136\n",
       "              -0.0879\n",
       "              -0.0447\n",
       "               0.0683\n",
       "              -0.0922\n",
       "              -0.0092\n",
       "              -0.1087\n",
       "              -0.0852\n",
       "               0.0103\n",
       "               0.0014\n",
       "               0.0176\n",
       "              -0.0963\n",
       "               0.0185\n",
       "              -0.0053\n",
       "               0.0501\n",
       "               0.0728\n",
       "              -0.0335\n",
       "              -0.1150\n",
       "              -0.1648\n",
       "              -0.0220\n",
       "              -0.0289\n",
       "               0.0139\n",
       "              -0.1039\n",
       "              -0.0741\n",
       "               0.0092\n",
       "              -0.0912\n",
       "              -0.0914\n",
       "              -0.0068\n",
       "               0.0610\n",
       "               0.0184\n",
       "               0.0067\n",
       "              -0.0776\n",
       "              -0.0346\n",
       "               0.0214\n",
       "              -0.1469\n",
       "              -0.0419\n",
       "               0.0296\n",
       "              -0.0587\n",
       "               0.0277\n",
       "              -0.0353\n",
       "              -0.0218\n",
       "              -0.0664\n",
       "              -0.0432\n",
       "               0.0082\n",
       "               0.0382\n",
       "              -0.0289\n",
       "              -0.0288\n",
       "              -0.0812\n",
       "              -0.1414\n",
       "               0.0028\n",
       "              -0.0022\n",
       "              -0.0648\n",
       "              -0.0584\n",
       "               0.0222\n",
       "               0.0164\n",
       "              -0.0880\n",
       "              -0.0148\n",
       "              -0.1055\n",
       "              -0.0568\n",
       "               0.0838\n",
       "              -0.0114\n",
       "              -0.0247\n",
       "              -0.1419\n",
       "               0.0305\n",
       "               0.0075\n",
       "               0.0127\n",
       "               0.0204\n",
       "              -0.0438\n",
       "              -0.0007\n",
       "               0.0075\n",
       "               0.0355\n",
       "              -0.0172\n",
       "               0.0041\n",
       "               0.0252\n",
       "               0.0155\n",
       "               0.0182\n",
       "              -0.0476\n",
       "              -0.0599\n",
       "               0.0885\n",
       "               0.0521\n",
       "              -0.0741\n",
       "               0.0378\n",
       "              -0.0536\n",
       "              -0.0835\n",
       "               0.0054\n",
       "              -0.0280\n",
       "              -0.0800\n",
       "              -0.1289\n",
       "              -0.0737\n",
       "               0.0711\n",
       "               0.0236\n",
       "              -0.1335\n",
       "              -0.0784\n",
       "              -0.0660\n",
       "              -0.0776\n",
       "              -0.0651\n",
       "              -0.0585\n",
       "               0.0293\n",
       "               0.0391\n",
       "              -0.0813\n",
       "               0.0235\n",
       "               0.0120\n",
       "              -0.0102\n",
       "               0.0082\n",
       "               0.0490\n",
       "               0.0905\n",
       "              -0.1098\n",
       "              -0.0345\n",
       "               0.0713\n",
       "              -0.1170\n",
       "              -0.0505\n",
       "               0.0617\n",
       "              -0.0692\n",
       "              -0.1186\n",
       "              -0.0833\n",
       "               0.0326\n",
       "               0.0317\n",
       "              -0.1941\n",
       "               0.0634\n",
       "              -0.1009\n",
       "              -0.1275\n",
       "              -0.0278\n",
       "               0.0315\n",
       "               0.0374\n",
       "              -0.0289\n",
       "               0.0031\n",
       "               0.0202\n",
       "              -0.0712\n",
       "               0.0293\n",
       "               0.0763\n",
       "               0.0364\n",
       "               0.0078\n",
       "               0.0518\n",
       "              -0.0838\n",
       "               0.0242\n",
       "              -0.0724\n",
       "              -0.0451\n",
       "              -0.0212\n",
       "              -0.0316\n",
       "              -0.0785\n",
       "              -0.1369\n",
       "              -0.0842\n",
       "               0.0627\n",
       "               0.0417\n",
       "              -0.1641\n",
       "               0.0118\n",
       "              -0.0702\n",
       "              -0.0077\n",
       "              -0.1031\n",
       "              -0.0093\n",
       "              -0.0402\n",
       "              -0.0580\n",
       "              -0.0367\n",
       "              -0.0677\n",
       "               0.0025\n",
       "               0.0227\n",
       "              -0.1306\n",
       "              -0.1558\n",
       "               0.0951\n",
       "               0.0331\n",
       "               0.0429\n",
       "              -0.0079\n",
       "              -0.0535\n",
       "               0.0235\n",
       "               0.0759\n",
       "              -0.0081\n",
       "               0.0197\n",
       "               0.0412\n",
       "              -0.0166\n",
       "               0.0909\n",
       "               0.0169\n",
       "               0.0581\n",
       "              -0.0918\n",
       "              -0.1014\n",
       "              -0.0927\n",
       "              -0.0598\n",
       "               0.0717\n",
       "              -0.0545\n",
       "               0.0123\n",
       "              -0.0245\n",
       "              -0.0084\n",
       "              -0.0772\n",
       "              -0.0906\n",
       "               0.0266\n",
       "              -0.0712\n",
       "              -0.0373\n",
       "               0.0441\n",
       "              -0.0631\n",
       "               0.0182\n",
       "              -0.0368\n",
       "              -0.0416\n",
       "               0.0457\n",
       "              -0.0583\n",
       "              -0.1104\n",
       "              -0.0533\n",
       "              -0.0172\n",
       "              -0.1110\n",
       "              -0.0212\n",
       "              -0.0328\n",
       "              -0.0892\n",
       "              -0.0424\n",
       "               0.1543\n",
       "              -0.0119\n",
       "              -0.0046\n",
       "              -0.0471\n",
       "              -0.0532\n",
       "               0.0120\n",
       "              -0.1729\n",
       "              -0.0237\n",
       "              -0.0956\n",
       "              -0.0654\n",
       "              -0.0770\n",
       "               0.0073\n",
       "              -0.1013\n",
       "               0.0338\n",
       "               0.0620\n",
       "              -0.0536\n",
       "              -0.0426\n",
       "              -0.0020\n",
       "              -0.1395\n",
       "               0.0039\n",
       "              -0.1031\n",
       "              -0.0414\n",
       "               0.0373\n",
       "              -0.1094\n",
       "               0.1056\n",
       "               0.0381\n",
       "              -0.0014\n",
       "              -0.1010\n",
       "              -0.0448\n",
       "               0.0051\n",
       "              -0.0456\n",
       "              -0.1280\n",
       "              -0.0613\n",
       "              -0.0494\n",
       "              -0.0923\n",
       "              -0.0162\n",
       "               0.0117\n",
       "               0.0037\n",
       "              -0.0453\n",
       "              -0.0123\n",
       "               0.0114\n",
       "              -0.0186\n",
       "               0.0361\n",
       "              -0.0371\n",
       "              -0.0343\n",
       "              -0.0297\n",
       "              -0.1554\n",
       "               0.0159\n",
       "              -0.0654\n",
       "              -0.0933\n",
       "              -0.0160\n",
       "               0.0424\n",
       "              -0.0421\n",
       "              -0.0120\n",
       "              -0.0789\n",
       "              -0.0728\n",
       "              -0.1051\n",
       "               0.0193\n",
       "              -0.0141\n",
       "              -0.1695\n",
       "              -0.0256\n",
       "               0.0116\n",
       "              -0.1649\n",
       "              -0.0365\n",
       "              -0.0495\n",
       "              -0.0482\n",
       "               0.0168\n",
       "              -0.0049\n",
       "               0.0560\n",
       "              -0.0802\n",
       "              -0.0741\n",
       "              -0.0416\n",
       "               0.0110\n",
       "               0.0607\n",
       "               0.0887\n",
       "              -0.0013\n",
       "              -0.0417\n",
       "               0.0429\n",
       "              -0.0171\n",
       "              -0.0795\n",
       "              -0.0648\n",
       "              -0.0259\n",
       "              -0.0300\n",
       "               0.0469\n",
       "              -0.0527\n",
       "              -0.1445\n",
       "              -0.1076\n",
       "              -0.0257\n",
       "               0.1150\n",
       "               0.0036\n",
       "              -0.1075\n",
       "              -0.1293\n",
       "              [torch.FloatTensor of size 512]), ('encode_b.1', \n",
       "              1.00000e-02 *\n",
       "               -6.5810\n",
       "                0.0756\n",
       "               -2.4533\n",
       "               -2.8183\n",
       "               -2.8073\n",
       "               -0.8343\n",
       "               -3.7315\n",
       "                1.8266\n",
       "               -0.7299\n",
       "               -0.1418\n",
       "               -2.3143\n",
       "               -1.2485\n",
       "               -2.6694\n",
       "                2.8140\n",
       "               -4.7344\n",
       "                0.3297\n",
       "                4.3436\n",
       "                1.2919\n",
       "                1.0702\n",
       "                1.9386\n",
       "               -2.4573\n",
       "               -2.9738\n",
       "                1.6354\n",
       "               -3.6295\n",
       "                0.0617\n",
       "                1.5022\n",
       "                0.8766\n",
       "               -4.9416\n",
       "                0.0028\n",
       "               -6.8008\n",
       "               -0.5546\n",
       "                1.3903\n",
       "               -1.2695\n",
       "                1.6100\n",
       "                0.7589\n",
       "               -0.8041\n",
       "               -0.1496\n",
       "               -0.0110\n",
       "               -3.8799\n",
       "                1.4606\n",
       "               -1.8710\n",
       "               -4.3148\n",
       "               -2.0453\n",
       "               -1.0468\n",
       "               -3.1643\n",
       "                1.0572\n",
       "               -5.9016\n",
       "               -0.1477\n",
       "               -2.7089\n",
       "               -0.7716\n",
       "               -0.3847\n",
       "               -3.2135\n",
       "                1.2508\n",
       "               -1.3113\n",
       "                2.0957\n",
       "               -1.6932\n",
       "                1.2015\n",
       "                0.7371\n",
       "               -3.0372\n",
       "               -6.1664\n",
       "               -0.3043\n",
       "               -1.1748\n",
       "               -1.1087\n",
       "               -2.8511\n",
       "               -2.3672\n",
       "               -3.7787\n",
       "               -1.8288\n",
       "               -3.3527\n",
       "               -2.4389\n",
       "                0.1505\n",
       "               -3.9641\n",
       "               -4.0390\n",
       "               -0.6190\n",
       "                1.7870\n",
       "                0.2681\n",
       "                1.3199\n",
       "               -0.4430\n",
       "               -2.4957\n",
       "                1.7004\n",
       "               -6.9125\n",
       "                1.2038\n",
       "               -0.1043\n",
       "                1.9616\n",
       "                1.5551\n",
       "               -2.1932\n",
       "                2.6860\n",
       "                0.2276\n",
       "               -0.6932\n",
       "                2.6849\n",
       "               -2.0092\n",
       "               -0.3040\n",
       "                0.8966\n",
       "               -4.9948\n",
       "               -3.3754\n",
       "               -1.5504\n",
       "               -3.1046\n",
       "               -6.2525\n",
       "               -3.0942\n",
       "               -2.9573\n",
       "               -4.1407\n",
       "               -5.4540\n",
       "                0.9769\n",
       "                1.0237\n",
       "               -2.9370\n",
       "               -0.9355\n",
       "                0.0003\n",
       "               -2.6506\n",
       "               -0.5279\n",
       "               -2.3634\n",
       "                1.2834\n",
       "               -2.5579\n",
       "                1.4388\n",
       "               -0.8425\n",
       "               -1.4435\n",
       "               -0.8677\n",
       "                2.0205\n",
       "                0.7931\n",
       "               -9.0266\n",
       "               -1.5309\n",
       "               -1.9467\n",
       "               -2.0755\n",
       "                0.7276\n",
       "               -3.5245\n",
       "                1.3008\n",
       "               -0.4336\n",
       "               -1.6042\n",
       "               -1.9274\n",
       "               -1.7890\n",
       "               -4.9644\n",
       "               -3.1649\n",
       "                0.1295\n",
       "               -5.8847\n",
       "                0.2683\n",
       "                0.0857\n",
       "               -1.0591\n",
       "               -2.7522\n",
       "               -2.9075\n",
       "                3.4883\n",
       "               -0.7517\n",
       "                0.7591\n",
       "               -2.3849\n",
       "               -0.8052\n",
       "                0.0508\n",
       "                1.5948\n",
       "                1.2033\n",
       "                1.6282\n",
       "                1.6184\n",
       "                2.2710\n",
       "                0.2574\n",
       "                0.6124\n",
       "               -1.8258\n",
       "               -2.3604\n",
       "                0.3140\n",
       "                0.6836\n",
       "                3.0627\n",
       "               -2.5798\n",
       "               -2.5621\n",
       "                1.1372\n",
       "                0.5605\n",
       "                0.3465\n",
       "               -0.0855\n",
       "               -2.0649\n",
       "               -4.4776\n",
       "               -4.0717\n",
       "               -2.9066\n",
       "               -1.0238\n",
       "               -2.6142\n",
       "               -2.9950\n",
       "                0.1723\n",
       "                1.8479\n",
       "               -0.8097\n",
       "                3.1318\n",
       "               -2.0252\n",
       "               -7.5156\n",
       "               -0.5838\n",
       "               -5.2737\n",
       "               -5.5666\n",
       "               -2.7205\n",
       "               -1.8848\n",
       "               -2.4632\n",
       "                0.7736\n",
       "               -3.0128\n",
       "                3.1876\n",
       "                0.6025\n",
       "               -1.5919\n",
       "               -5.7608\n",
       "               -3.6002\n",
       "               -2.2437\n",
       "               -0.3350\n",
       "                1.0059\n",
       "               -0.5298\n",
       "               -1.6315\n",
       "               -5.6797\n",
       "               -1.1683\n",
       "               -3.0601\n",
       "                0.2336\n",
       "               -2.8050\n",
       "                2.6862\n",
       "               -4.1457\n",
       "               -1.8277\n",
       "               -6.7654\n",
       "                3.4947\n",
       "                1.6414\n",
       "                0.8558\n",
       "                0.6564\n",
       "                0.5812\n",
       "               -1.8368\n",
       "                0.3895\n",
       "               -2.2397\n",
       "                0.0832\n",
       "               -1.8171\n",
       "               -0.0727\n",
       "               -2.3101\n",
       "                0.3823\n",
       "               -2.0272\n",
       "                1.3578\n",
       "               -0.5073\n",
       "                1.1903\n",
       "                0.5503\n",
       "               -3.6037\n",
       "               -2.7432\n",
       "               -1.1571\n",
       "               -3.4439\n",
       "               -0.3232\n",
       "               -1.0150\n",
       "               -0.1105\n",
       "               -0.9503\n",
       "                0.8757\n",
       "               -2.3876\n",
       "                0.3749\n",
       "                3.4602\n",
       "               -4.6271\n",
       "               -3.1840\n",
       "                0.0460\n",
       "               -2.9620\n",
       "               -2.9699\n",
       "               -1.6162\n",
       "               -1.2277\n",
       "               -0.1550\n",
       "                2.1739\n",
       "               -2.4822\n",
       "                1.0637\n",
       "               -0.8970\n",
       "               -3.2840\n",
       "               -1.1963\n",
       "               -0.2232\n",
       "                0.0357\n",
       "               -2.4698\n",
       "                0.3671\n",
       "                1.8519\n",
       "                1.1068\n",
       "               -0.8957\n",
       "               -0.0273\n",
       "               -0.6330\n",
       "                0.1414\n",
       "               -2.8359\n",
       "               -0.0389\n",
       "                1.2393\n",
       "                1.3573\n",
       "               -1.3439\n",
       "               -1.4306\n",
       "               -0.4474\n",
       "               -2.5377\n",
       "                0.0064\n",
       "                1.6605\n",
       "                1.6942\n",
       "               -2.5876\n",
       "                0.0859\n",
       "                1.8834\n",
       "                0.8677\n",
       "               -4.6703\n",
       "               -4.9557\n",
       "                2.7071\n",
       "                0.4629\n",
       "                4.7592\n",
       "               -0.9522\n",
       "                0.6755\n",
       "               -1.9035\n",
       "               -3.3203\n",
       "               -2.4531\n",
       "               -5.5330\n",
       "               -0.5209\n",
       "               -3.6596\n",
       "                1.5876\n",
       "               -5.8999\n",
       "               -6.2741\n",
       "                1.8922\n",
       "                0.3614\n",
       "               -2.1775\n",
       "               -1.7303\n",
       "                0.1819\n",
       "               -0.1273\n",
       "                0.4725\n",
       "                0.2209\n",
       "               -2.4335\n",
       "               -1.4693\n",
       "               -3.0504\n",
       "               -2.7069\n",
       "                0.0110\n",
       "                1.1164\n",
       "               -0.8790\n",
       "               -0.4330\n",
       "                0.7957\n",
       "               -0.6329\n",
       "               -1.6927\n",
       "                0.6715\n",
       "               -1.7631\n",
       "               -2.9298\n",
       "               -1.6590\n",
       "               -2.6015\n",
       "               -2.2260\n",
       "                3.6812\n",
       "                0.7051\n",
       "                1.4605\n",
       "               -1.0592\n",
       "               -2.6565\n",
       "               -1.6251\n",
       "               -1.2074\n",
       "               -1.3547\n",
       "               -1.9992\n",
       "               -1.2240\n",
       "               -2.2401\n",
       "               -4.0838\n",
       "                2.1907\n",
       "                1.3736\n",
       "                1.4711\n",
       "               -2.5244\n",
       "               -0.8749\n",
       "                1.2143\n",
       "                0.5283\n",
       "               -3.0621\n",
       "               -1.3543\n",
       "                1.0654\n",
       "                1.1317\n",
       "               -1.4664\n",
       "               -0.3644\n",
       "               -1.6782\n",
       "               -0.4855\n",
       "               -3.7214\n",
       "               -0.0823\n",
       "               -2.9518\n",
       "                0.7068\n",
       "                0.9488\n",
       "               -4.3604\n",
       "               -7.0113\n",
       "               -0.7583\n",
       "               -4.4068\n",
       "                2.4374\n",
       "                2.9257\n",
       "                2.4076\n",
       "               -0.1486\n",
       "               -0.5548\n",
       "               -3.3172\n",
       "               -0.4613\n",
       "                0.7810\n",
       "                2.2299\n",
       "                0.1958\n",
       "                2.6509\n",
       "               -3.2110\n",
       "               -2.3059\n",
       "                1.4711\n",
       "                0.8235\n",
       "                0.7299\n",
       "               -0.8299\n",
       "               -2.0443\n",
       "               -5.3430\n",
       "                0.3000\n",
       "               -6.5220\n",
       "               -2.2882\n",
       "               -1.6302\n",
       "               -3.6885\n",
       "               -1.8281\n",
       "               -1.2290\n",
       "               -3.8258\n",
       "               -0.0590\n",
       "                0.7813\n",
       "                0.5133\n",
       "               -1.1446\n",
       "               -4.3363\n",
       "               -2.1967\n",
       "               -1.0560\n",
       "                0.8913\n",
       "               -0.7029\n",
       "                2.4413\n",
       "                1.7349\n",
       "               -1.3702\n",
       "               -0.8426\n",
       "               -0.8816\n",
       "               -0.2267\n",
       "               -1.7228\n",
       "               -0.0911\n",
       "                1.0433\n",
       "               -1.4970\n",
       "               -1.0609\n",
       "               -4.2968\n",
       "               -0.7203\n",
       "               -4.5161\n",
       "               -0.7146\n",
       "                2.9197\n",
       "                0.6488\n",
       "               -0.4557\n",
       "               -1.0391\n",
       "                1.0903\n",
       "                0.6355\n",
       "                0.1019\n",
       "               -1.1297\n",
       "               -1.3296\n",
       "                6.1261\n",
       "               -7.4343\n",
       "               -1.8748\n",
       "                0.7944\n",
       "               -2.9306\n",
       "               -0.8279\n",
       "               -1.2049\n",
       "               -3.5841\n",
       "               -5.1779\n",
       "               -2.9581\n",
       "               -1.4185\n",
       "               -0.0951\n",
       "                0.5078\n",
       "               -5.6728\n",
       "               -1.9531\n",
       "               -1.4103\n",
       "               -5.3157\n",
       "                1.2787\n",
       "                0.2698\n",
       "               -4.1554\n",
       "               -0.8680\n",
       "                0.1139\n",
       "               -0.7501\n",
       "               -0.6001\n",
       "               -3.1251\n",
       "                0.9926\n",
       "                4.4067\n",
       "               -0.6176\n",
       "               -4.8715\n",
       "               -1.7853\n",
       "                0.9225\n",
       "               -1.3682\n",
       "               -2.3180\n",
       "                1.5625\n",
       "               -0.9391\n",
       "               -0.0591\n",
       "               -4.2874\n",
       "               -0.8292\n",
       "                0.2499\n",
       "               -1.3903\n",
       "                1.5293\n",
       "                0.3553\n",
       "                0.6559\n",
       "               -7.8386\n",
       "               -1.4067\n",
       "                1.1849\n",
       "               -4.5706\n",
       "                0.1337\n",
       "                1.0666\n",
       "                4.9147\n",
       "                0.4621\n",
       "               -2.9699\n",
       "               -2.5250\n",
       "               -0.3394\n",
       "                0.7827\n",
       "               -3.2491\n",
       "                0.1200\n",
       "               -3.2741\n",
       "                2.1452\n",
       "                0.1399\n",
       "               -4.2095\n",
       "               -0.6857\n",
       "                0.3504\n",
       "               -1.3844\n",
       "               -1.4738\n",
       "               -0.7625\n",
       "               -2.8036\n",
       "               -0.1790\n",
       "                1.5898\n",
       "               -0.9759\n",
       "               -2.3572\n",
       "                4.7330\n",
       "                1.5135\n",
       "                2.1486\n",
       "               -0.2852\n",
       "               -0.0794\n",
       "               -2.2255\n",
       "                0.1937\n",
       "               -0.3787\n",
       "                0.5648\n",
       "               -1.8485\n",
       "                2.5439\n",
       "                3.9558\n",
       "               -3.0150\n",
       "               -1.3574\n",
       "                1.1917\n",
       "               -0.7216\n",
       "               -0.1452\n",
       "               -1.3118\n",
       "               -0.3897\n",
       "               -0.7848\n",
       "               -2.3779\n",
       "               -0.7788\n",
       "               -0.7016\n",
       "               -2.6575\n",
       "                2.3706\n",
       "               -0.5011\n",
       "                1.5172\n",
       "                0.2409\n",
       "                0.6501\n",
       "               -0.3119\n",
       "                1.3582\n",
       "               -0.5784\n",
       "                1.5409\n",
       "               -0.0263\n",
       "              [torch.FloatTensor of size 512]), ('encode_b.2', \n",
       "              1.00000e-02 *\n",
       "                0.9627\n",
       "               -1.4945\n",
       "               -1.9627\n",
       "                  ⋮   \n",
       "               -0.7493\n",
       "               -1.1248\n",
       "               -0.6257\n",
       "              [torch.FloatTensor of size 1024]), ('decode_w.0', \n",
       "              -5.9568e-02 -1.3026e-02 -7.6969e-04  ...  -5.9007e-03 -4.1451e-02  4.4183e-02\n",
       "               1.3580e-02  1.5489e-04 -1.5657e-02  ...   5.6324e-02 -4.2011e-02 -3.9864e-02\n",
       "              -5.9498e-02 -1.1119e-03  4.6796e-02  ...   5.5258e-02 -2.2846e-02  2.8780e-02\n",
       "                              ...                   ⋱                   ...                \n",
       "               1.5233e-02  2.7413e-02 -2.8045e-02  ...  -1.1169e-03  2.2642e-03  3.1062e-02\n",
       "               2.6824e-03  9.6018e-03  2.7540e-02  ...   4.3340e-02 -5.9744e-03 -2.3525e-02\n",
       "              -1.8576e-01 -4.8126e-02 -9.5664e-03  ...  -2.1752e-02 -8.2946e-02  2.4299e-02\n",
       "              [torch.FloatTensor of size 512x1024]), ('decode_w.1', \n",
       "              -1.4338e-02 -2.2930e-02  3.9534e-02  ...   1.1826e-02 -1.1537e-02 -6.7008e-02\n",
       "              -4.1934e-02 -4.1094e-02 -1.9725e-03  ...   7.5139e-02 -9.0207e-03 -6.5803e-02\n",
       "              -5.0370e-02 -2.4934e-03  3.2086e-02  ...   9.3499e-03  7.1137e-02 -3.9892e-02\n",
       "                              ...                   ⋱                   ...                \n",
       "              -6.2580e-02 -6.0335e-02  6.6407e-02  ...  -2.1925e-02  2.8672e-02  2.0197e-02\n",
       "               2.1349e-02  6.3161e-02  2.9311e-02  ...   5.2447e-02 -4.5120e-02 -9.2806e-03\n",
       "               5.1809e-02 -5.1835e-03  2.3455e-02  ...   3.3614e-02  9.4280e-03 -3.3504e-02\n",
       "              [torch.FloatTensor of size 512x512]), ('decode_w.2', \n",
       "              -1.8932e-02  9.6624e-03  2.2053e-03  ...  -9.5304e-04 -1.8385e-02 -5.5507e-03\n",
       "              -1.7092e-02  6.8164e-04  7.1383e-03  ...   8.4627e-04  1.3302e-03  3.0461e-03\n",
       "               1.5988e-03  8.9567e-03 -5.2419e-03  ...  -1.5653e-02 -8.0868e-03 -1.5089e-03\n",
       "                              ...                   ⋱                   ...                \n",
       "               1.1941e-03 -2.9976e-03  1.6109e-02  ...  -4.3899e-03 -1.1942e-02  1.0278e-02\n",
       "               1.3250e-02 -3.6165e-03 -9.7513e-03  ...   9.4815e-04 -1.0151e-03  7.4349e-03\n",
       "               1.4230e-02  1.2505e-02 -2.1880e-03  ...   3.8670e-03  2.4848e-03 -1.1511e-02\n",
       "              [torch.FloatTensor of size 17736x512]), ('decode_b.0', \n",
       "              1.00000e-02 *\n",
       "               -2.0509\n",
       "                0.6559\n",
       "               -2.5808\n",
       "               -3.2439\n",
       "               -0.0749\n",
       "               -0.8147\n",
       "                1.4150\n",
       "               -2.9429\n",
       "               -1.7123\n",
       "               -0.3672\n",
       "                3.8026\n",
       "                4.7430\n",
       "                0.1342\n",
       "                1.2088\n",
       "                0.1472\n",
       "               -1.0546\n",
       "                0.2483\n",
       "                0.2097\n",
       "               -0.7550\n",
       "                3.4674\n",
       "               -2.1528\n",
       "                0.6515\n",
       "                0.1017\n",
       "               -2.5370\n",
       "                1.6201\n",
       "                0.2696\n",
       "               -1.8819\n",
       "               -2.0004\n",
       "               -0.6236\n",
       "                0.0506\n",
       "               -3.8293\n",
       "                3.3748\n",
       "               -2.9162\n",
       "                2.5455\n",
       "               -2.5921\n",
       "               -2.3733\n",
       "                3.7404\n",
       "                0.6533\n",
       "               -0.2200\n",
       "               -1.3461\n",
       "               -1.2701\n",
       "               -3.0087\n",
       "               -0.9065\n",
       "                0.1973\n",
       "               -3.6792\n",
       "               -1.4720\n",
       "               -1.6624\n",
       "               -1.9081\n",
       "               -1.8078\n",
       "               -0.2886\n",
       "               -2.3298\n",
       "                3.1680\n",
       "               -0.4620\n",
       "               -0.6687\n",
       "                0.2465\n",
       "               -1.5242\n",
       "                1.5034\n",
       "               -1.3967\n",
       "               -0.2920\n",
       "                0.4319\n",
       "               -3.5089\n",
       "               -1.3040\n",
       "               -0.1976\n",
       "                0.3569\n",
       "               -1.9773\n",
       "               -2.7888\n",
       "               -1.4139\n",
       "               -3.0752\n",
       "               -1.3496\n",
       "               -0.9215\n",
       "               -2.4490\n",
       "                1.6469\n",
       "                0.5197\n",
       "                0.0137\n",
       "               -3.4483\n",
       "                2.5073\n",
       "               -2.8039\n",
       "                0.6872\n",
       "                0.5738\n",
       "               -3.3947\n",
       "               -2.2001\n",
       "                0.4710\n",
       "               -2.7786\n",
       "               -1.2236\n",
       "                3.0070\n",
       "                0.9676\n",
       "                2.4577\n",
       "               -1.5867\n",
       "               -2.4286\n",
       "                1.1297\n",
       "               -2.3679\n",
       "               -3.4229\n",
       "               -0.4956\n",
       "               -3.1697\n",
       "                1.3487\n",
       "               -1.6733\n",
       "               -2.3706\n",
       "               -0.6536\n",
       "               -2.8052\n",
       "                2.6712\n",
       "               -1.0541\n",
       "               -1.4782\n",
       "               -1.3928\n",
       "               -0.6481\n",
       "               -2.5372\n",
       "               -3.5510\n",
       "               -0.4268\n",
       "               -0.3790\n",
       "               -2.8962\n",
       "               -0.4159\n",
       "               -3.5860\n",
       "                1.4111\n",
       "               -3.7579\n",
       "               -3.9522\n",
       "                0.5827\n",
       "               -2.4613\n",
       "               -1.2703\n",
       "               -1.6680\n",
       "                3.1452\n",
       "                1.9899\n",
       "               -1.1701\n",
       "               -0.5780\n",
       "               -3.8771\n",
       "               -3.2749\n",
       "                3.1026\n",
       "               -0.9512\n",
       "               -2.8185\n",
       "               -3.1204\n",
       "               -3.3582\n",
       "               -2.0777\n",
       "                2.6835\n",
       "               -1.1476\n",
       "                0.4835\n",
       "               -2.8047\n",
       "               -3.3136\n",
       "               -0.5070\n",
       "               -3.4151\n",
       "                1.1535\n",
       "               -3.2019\n",
       "               -2.2286\n",
       "               -1.4464\n",
       "                0.8903\n",
       "               -3.4775\n",
       "               -3.2290\n",
       "               -3.2493\n",
       "                0.0884\n",
       "               -2.4051\n",
       "               -3.1384\n",
       "               -2.6393\n",
       "               -1.2217\n",
       "               -2.8816\n",
       "               -0.6768\n",
       "                0.8944\n",
       "               -3.2070\n",
       "               -2.3582\n",
       "               -1.7306\n",
       "               -0.6240\n",
       "                2.9000\n",
       "               -2.9119\n",
       "               -3.1878\n",
       "               -0.7440\n",
       "               -0.0029\n",
       "               -0.5470\n",
       "               -3.5315\n",
       "               -2.2334\n",
       "               -1.9227\n",
       "               -2.9500\n",
       "               -0.9680\n",
       "               -2.0607\n",
       "               -2.3158\n",
       "                2.4842\n",
       "               -0.8168\n",
       "               -0.1915\n",
       "                5.6366\n",
       "                3.0322\n",
       "               -3.3652\n",
       "                2.8482\n",
       "                1.8724\n",
       "               -0.6611\n",
       "               -3.2619\n",
       "                5.1732\n",
       "                4.0627\n",
       "               -2.4718\n",
       "                1.3378\n",
       "               -3.7030\n",
       "               -2.3661\n",
       "                0.1386\n",
       "               -0.7466\n",
       "                0.3755\n",
       "               -1.0050\n",
       "               -1.9167\n",
       "                1.8264\n",
       "               -1.3298\n",
       "               -0.6278\n",
       "               -0.9441\n",
       "                4.5426\n",
       "               -0.3886\n",
       "                3.9513\n",
       "               -0.8729\n",
       "               -1.1040\n",
       "               -0.3244\n",
       "                1.1711\n",
       "               -2.5242\n",
       "               -2.3135\n",
       "                2.3258\n",
       "               -0.8551\n",
       "                1.3839\n",
       "               -0.7516\n",
       "               -1.3939\n",
       "                0.7044\n",
       "               -0.5119\n",
       "                2.5627\n",
       "                1.6188\n",
       "                2.6171\n",
       "                0.9708\n",
       "               -2.1353\n",
       "                2.4562\n",
       "               -3.1580\n",
       "                0.0970\n",
       "               -0.5758\n",
       "               -0.5873\n",
       "                0.8096\n",
       "               -2.1559\n",
       "                2.4875\n",
       "               -1.1234\n",
       "               -0.3814\n",
       "               -2.6044\n",
       "               -1.6500\n",
       "                4.1633\n",
       "               -3.3885\n",
       "               -0.7039\n",
       "               -2.9472\n",
       "                1.0984\n",
       "                1.6591\n",
       "                0.1473\n",
       "                1.4895\n",
       "               -2.2864\n",
       "               -2.6801\n",
       "               -2.3712\n",
       "               -0.7783\n",
       "               -0.8030\n",
       "               -1.2175\n",
       "               -2.3863\n",
       "                0.2282\n",
       "               -2.3117\n",
       "               -0.8320\n",
       "               -3.4731\n",
       "                0.4722\n",
       "               -3.5261\n",
       "                2.0910\n",
       "               -0.8304\n",
       "               -3.2489\n",
       "               -1.0168\n",
       "                4.4736\n",
       "               -1.4812\n",
       "               -1.2326\n",
       "               -2.7747\n",
       "               -0.1964\n",
       "               -2.5848\n",
       "               -1.9029\n",
       "               -3.3012\n",
       "               -3.0500\n",
       "               -1.9001\n",
       "               -0.7823\n",
       "               -0.1444\n",
       "                0.9462\n",
       "               -1.6089\n",
       "               -2.8152\n",
       "               -1.3482\n",
       "                1.4049\n",
       "               -0.6502\n",
       "               -1.3423\n",
       "               -0.4355\n",
       "                3.4903\n",
       "               -1.5008\n",
       "               -1.5493\n",
       "                0.5124\n",
       "                1.6672\n",
       "                3.1756\n",
       "               -2.2178\n",
       "               -1.3459\n",
       "               -2.0133\n",
       "               -2.7728\n",
       "               -3.2889\n",
       "                0.2973\n",
       "                1.8142\n",
       "               -0.7950\n",
       "                0.4540\n",
       "                2.0190\n",
       "               -1.6963\n",
       "               -0.7507\n",
       "               -1.4273\n",
       "               -1.0683\n",
       "               -0.3284\n",
       "               -1.0970\n",
       "               -1.7781\n",
       "               -2.9340\n",
       "               -3.3419\n",
       "               -3.3075\n",
       "               -2.1926\n",
       "               -2.4675\n",
       "               -2.1367\n",
       "               -2.0936\n",
       "                1.3416\n",
       "               -3.6163\n",
       "               -3.2224\n",
       "               -0.4477\n",
       "               -0.0744\n",
       "               -0.3682\n",
       "               -1.7930\n",
       "               -1.7163\n",
       "               -3.4530\n",
       "                2.2673\n",
       "               -1.2307\n",
       "                3.6721\n",
       "               -1.9309\n",
       "                2.3018\n",
       "               -1.8615\n",
       "               -0.6081\n",
       "               -1.5756\n",
       "                0.8283\n",
       "               -0.0058\n",
       "                1.2120\n",
       "               -1.2744\n",
       "               -0.8369\n",
       "               -3.1752\n",
       "               -0.4896\n",
       "               -3.2742\n",
       "               -2.7873\n",
       "                3.3489\n",
       "                0.4314\n",
       "               -2.1231\n",
       "               -1.8066\n",
       "                0.3487\n",
       "                0.8481\n",
       "                0.0851\n",
       "               -1.9448\n",
       "               -2.9849\n",
       "               -2.3585\n",
       "               -3.2373\n",
       "                1.8199\n",
       "                1.8770\n",
       "               -2.6613\n",
       "               -1.2420\n",
       "               -1.1878\n",
       "                4.6629\n",
       "                6.0709\n",
       "               -0.2308\n",
       "                0.4206\n",
       "               -1.9383\n",
       "                4.1927\n",
       "               -2.9725\n",
       "               -0.8946\n",
       "               -1.3255\n",
       "                0.0298\n",
       "                2.8199\n",
       "               -3.5518\n",
       "                1.6886\n",
       "                1.9817\n",
       "                0.4322\n",
       "                0.8674\n",
       "               -4.0945\n",
       "                0.2684\n",
       "               -2.6247\n",
       "               -0.7956\n",
       "               -2.3830\n",
       "               -2.1470\n",
       "               -1.9746\n",
       "               -2.9791\n",
       "                1.7553\n",
       "               -1.6756\n",
       "                4.1259\n",
       "                2.0787\n",
       "                1.7694\n",
       "               -0.0033\n",
       "               -0.5639\n",
       "               -3.3083\n",
       "                1.9586\n",
       "               -0.8313\n",
       "                0.7641\n",
       "                3.8428\n",
       "                1.6327\n",
       "               -2.2203\n",
       "               -3.5952\n",
       "                0.2903\n",
       "               -0.6044\n",
       "                3.7652\n",
       "                0.7372\n",
       "               -1.3504\n",
       "               -2.6106\n",
       "               -2.5872\n",
       "               -1.8635\n",
       "                0.1635\n",
       "                4.7670\n",
       "               -0.6378\n",
       "               -3.1915\n",
       "                3.3314\n",
       "               -1.6324\n",
       "                0.9929\n",
       "               -2.8501\n",
       "               -2.9631\n",
       "               -2.9030\n",
       "                1.8590\n",
       "                1.9082\n",
       "               -1.3736\n",
       "               -0.6667\n",
       "               -0.9621\n",
       "               -1.2688\n",
       "                4.7072\n",
       "                2.8127\n",
       "               -0.5035\n",
       "               -0.9957\n",
       "               -1.7158\n",
       "               -2.3038\n",
       "               -0.8598\n",
       "                2.0567\n",
       "               -2.4174\n",
       "                1.5068\n",
       "               -2.3141\n",
       "                0.5749\n",
       "               -1.9858\n",
       "               -2.0131\n",
       "                1.3190\n",
       "                0.3633\n",
       "               -0.8853\n",
       "                2.3159\n",
       "               -1.1270\n",
       "               -0.1802\n",
       "               -2.0556\n",
       "               -0.2289\n",
       "                0.0763\n",
       "               -0.1560\n",
       "               -3.0694\n",
       "               -0.5150\n",
       "                1.1262\n",
       "               -1.4484\n",
       "               -0.3797\n",
       "               -3.1669\n",
       "               -2.8817\n",
       "               -0.4510\n",
       "               -1.3724\n",
       "               -3.2068\n",
       "               -3.0788\n",
       "               -2.0821\n",
       "               -1.1407\n",
       "               -1.6947\n",
       "               -1.7521\n",
       "               -0.3004\n",
       "               -1.2213\n",
       "                1.6773\n",
       "                1.6759\n",
       "               -1.9321\n",
       "               -0.6726\n",
       "                0.0802\n",
       "               -0.4173\n",
       "               -1.9472\n",
       "               -3.2050\n",
       "               -3.1015\n",
       "                0.8457\n",
       "               -1.5658\n",
       "                0.0873\n",
       "               -2.0799\n",
       "               -2.2649\n",
       "               -0.9375\n",
       "               -0.4102\n",
       "                1.7738\n",
       "                0.0525\n",
       "                4.1428\n",
       "               -0.7985\n",
       "               -2.7346\n",
       "               -3.4567\n",
       "               -1.3990\n",
       "               -3.2572\n",
       "               -0.6447\n",
       "               -3.5451\n",
       "               -1.4189\n",
       "               -0.6790\n",
       "               -0.2260\n",
       "               -0.2886\n",
       "                1.6514\n",
       "               -2.7711\n",
       "               -1.5310\n",
       "               -0.0018\n",
       "               -1.7192\n",
       "               -1.1524\n",
       "               -2.2114\n",
       "               -1.8863\n",
       "                1.1511\n",
       "                0.0857\n",
       "               -0.1330\n",
       "               -1.5391\n",
       "               -1.4706\n",
       "               -3.1234\n",
       "                0.7302\n",
       "               -0.6780\n",
       "               -1.8669\n",
       "                1.1164\n",
       "               -1.4951\n",
       "               -0.3209\n",
       "               -2.1294\n",
       "               -3.3932\n",
       "                4.0466\n",
       "               -2.6999\n",
       "                0.0052\n",
       "               -1.8976\n",
       "               -1.0116\n",
       "               -3.2984\n",
       "               -1.6066\n",
       "               -3.2907\n",
       "               -0.9648\n",
       "                1.5292\n",
       "                3.4346\n",
       "              [torch.FloatTensor of size 512]), ('decode_b.1', \n",
       "              1.00000e-02 *\n",
       "               -0.9336\n",
       "               -0.4239\n",
       "               -0.8501\n",
       "               -0.4455\n",
       "                0.7465\n",
       "               -0.3252\n",
       "                2.0331\n",
       "               -0.1933\n",
       "                0.4472\n",
       "               -0.7485\n",
       "                0.0163\n",
       "               -0.8548\n",
       "               -0.6592\n",
       "               -0.3117\n",
       "               -0.7072\n",
       "               -0.6662\n",
       "                1.0081\n",
       "               -0.4467\n",
       "               -0.6148\n",
       "               -0.7596\n",
       "               -0.4854\n",
       "                0.6817\n",
       "               -0.4385\n",
       "               -0.5844\n",
       "               -0.8183\n",
       "                1.7445\n",
       "               -0.4803\n",
       "                0.8896\n",
       "               -0.7464\n",
       "               -0.5065\n",
       "                2.3136\n",
       "               -0.6626\n",
       "               -0.4752\n",
       "               -0.4215\n",
       "               -0.4241\n",
       "               -0.1658\n",
       "               -0.5437\n",
       "                1.0821\n",
       "               -0.7375\n",
       "                1.3387\n",
       "                0.6602\n",
       "               -0.5586\n",
       "                2.1300\n",
       "                0.0113\n",
       "                0.1358\n",
       "               -0.6440\n",
       "               -0.5757\n",
       "               -0.9496\n",
       "                2.6330\n",
       "               -0.6822\n",
       "                0.1556\n",
       "               -0.7294\n",
       "                0.0743\n",
       "                1.4451\n",
       "               -0.0436\n",
       "                1.5424\n",
       "               -0.6745\n",
       "               -0.7763\n",
       "                0.2094\n",
       "               -0.4436\n",
       "               -0.6406\n",
       "               -0.8616\n",
       "                0.9420\n",
       "               -0.8646\n",
       "               -0.3826\n",
       "               -1.0435\n",
       "               -0.0118\n",
       "                1.7254\n",
       "               -0.2536\n",
       "               -1.0224\n",
       "               -0.6697\n",
       "               -0.4859\n",
       "                3.4118\n",
       "               -0.5669\n",
       "               -0.5931\n",
       "               -0.8300\n",
       "               -0.6718\n",
       "               -0.8729\n",
       "                1.3298\n",
       "               -0.7720\n",
       "                2.1381\n",
       "                0.5831\n",
       "                1.7436\n",
       "               -0.5393\n",
       "               -0.6563\n",
       "                0.5701\n",
       "               -0.6800\n",
       "                1.9360\n",
       "               -0.7801\n",
       "               -0.5754\n",
       "               -0.3691\n",
       "               -0.8415\n",
       "                2.2214\n",
       "               -0.7366\n",
       "               -0.3787\n",
       "               -0.5604\n",
       "                1.2863\n",
       "               -0.6979\n",
       "               -0.7222\n",
       "               -0.8159\n",
       "               -0.4532\n",
       "                0.4705\n",
       "                0.0061\n",
       "               -0.5881\n",
       "               -0.6861\n",
       "               -0.2216\n",
       "               -0.6580\n",
       "                1.8372\n",
       "               -0.6825\n",
       "               -0.8539\n",
       "                2.0906\n",
       "               -0.5584\n",
       "               -0.7163\n",
       "               -0.5844\n",
       "               -0.5525\n",
       "               -0.4847\n",
       "               -0.5304\n",
       "                1.4092\n",
       "               -0.5276\n",
       "                2.4449\n",
       "               -0.8589\n",
       "               -0.5735\n",
       "               -0.4496\n",
       "                0.1718\n",
       "                1.5597\n",
       "                0.9181\n",
       "                0.7012\n",
       "               -0.5023\n",
       "                1.4954\n",
       "                1.7132\n",
       "               -0.8643\n",
       "               -0.5515\n",
       "               -0.4347\n",
       "               -0.9419\n",
       "                1.5457\n",
       "               -0.4455\n",
       "                1.0792\n",
       "               -0.5030\n",
       "                1.5758\n",
       "               -0.7029\n",
       "               -0.2870\n",
       "               -0.4335\n",
       "               -0.6727\n",
       "               -0.5244\n",
       "                2.5748\n",
       "               -0.5593\n",
       "                3.2225\n",
       "               -0.6118\n",
       "               -0.8238\n",
       "               -0.6088\n",
       "               -0.3493\n",
       "               -0.3734\n",
       "               -0.1849\n",
       "               -0.5580\n",
       "               -0.4376\n",
       "               -0.6662\n",
       "               -0.4627\n",
       "               -0.4751\n",
       "                1.8959\n",
       "                1.6423\n",
       "               -0.8143\n",
       "               -0.8534\n",
       "               -0.6141\n",
       "                0.9541\n",
       "               -0.4447\n",
       "               -0.7535\n",
       "               -0.9365\n",
       "                0.1994\n",
       "               -0.8800\n",
       "               -0.4765\n",
       "               -0.7762\n",
       "                2.0375\n",
       "               -0.4602\n",
       "                2.5429\n",
       "                0.9396\n",
       "               -0.4285\n",
       "                0.9812\n",
       "               -0.4030\n",
       "               -0.7581\n",
       "               -0.2340\n",
       "                0.0071\n",
       "               -0.2975\n",
       "               -0.2556\n",
       "                0.0333\n",
       "                1.1806\n",
       "               -0.4481\n",
       "               -0.7554\n",
       "               -0.0184\n",
       "               -0.1507\n",
       "                3.6515\n",
       "               -0.4944\n",
       "               -0.3508\n",
       "               -0.1764\n",
       "               -0.8024\n",
       "                1.0202\n",
       "                0.9466\n",
       "                0.0383\n",
       "                2.4339\n",
       "                0.0680\n",
       "                0.4964\n",
       "                3.0346\n",
       "               -0.5213\n",
       "               -0.8566\n",
       "               -0.4047\n",
       "               -0.5808\n",
       "                0.5242\n",
       "               -0.1183\n",
       "                2.3002\n",
       "                0.4938\n",
       "               -0.5328\n",
       "               -0.4761\n",
       "                0.7572\n",
       "                0.9058\n",
       "               -0.6136\n",
       "                0.9952\n",
       "               -0.4827\n",
       "               -0.3140\n",
       "                0.9362\n",
       "                0.2500\n",
       "                1.9371\n",
       "               -0.6231\n",
       "               -0.4293\n",
       "               -0.6694\n",
       "                1.5897\n",
       "               -0.3997\n",
       "               -0.3370\n",
       "               -0.6917\n",
       "               -0.6773\n",
       "               -0.9596\n",
       "               -0.1035\n",
       "                1.8512\n",
       "               -0.8327\n",
       "                1.4914\n",
       "                2.9779\n",
       "                0.3605\n",
       "               -0.5614\n",
       "                0.6998\n",
       "                1.2077\n",
       "               -0.6297\n",
       "               -0.3966\n",
       "                1.8785\n",
       "                1.3214\n",
       "                2.2777\n",
       "                1.4637\n",
       "               -0.3477\n",
       "                1.5366\n",
       "                1.5300\n",
       "               -0.5631\n",
       "               -0.4829\n",
       "                0.2048\n",
       "               -0.8148\n",
       "                0.3044\n",
       "                3.0448\n",
       "               -0.5044\n",
       "               -0.6706\n",
       "               -0.7214\n",
       "               -0.5251\n",
       "               -0.7569\n",
       "               -0.6678\n",
       "                1.0260\n",
       "               -0.6021\n",
       "               -0.4836\n",
       "               -0.5286\n",
       "                1.2117\n",
       "               -0.6837\n",
       "               -0.9340\n",
       "               -0.8044\n",
       "                2.0698\n",
       "               -0.4986\n",
       "                0.7493\n",
       "               -0.7426\n",
       "                2.2160\n",
       "                0.3836\n",
       "               -0.8290\n",
       "               -0.5934\n",
       "               -0.3753\n",
       "                1.6663\n",
       "                1.6304\n",
       "               -0.3761\n",
       "               -0.9757\n",
       "               -0.9404\n",
       "                2.0926\n",
       "               -0.7736\n",
       "               -0.8021\n",
       "               -0.6656\n",
       "               -0.6817\n",
       "               -0.5579\n",
       "                1.1932\n",
       "               -0.4375\n",
       "               -0.5763\n",
       "               -0.9048\n",
       "               -0.3157\n",
       "                0.9571\n",
       "                0.4894\n",
       "                1.2999\n",
       "               -0.4000\n",
       "                0.1740\n",
       "               -0.7678\n",
       "                3.0627\n",
       "               -0.5293\n",
       "                1.5533\n",
       "                2.0192\n",
       "               -0.6968\n",
       "                0.3525\n",
       "                0.0129\n",
       "                1.3393\n",
       "               -0.4442\n",
       "                2.8206\n",
       "               -0.4312\n",
       "               -0.7791\n",
       "                0.3144\n",
       "               -0.4220\n",
       "                1.2196\n",
       "                0.0890\n",
       "                0.9097\n",
       "               -0.2912\n",
       "                0.1496\n",
       "                0.3613\n",
       "               -0.3635\n",
       "               -0.5397\n",
       "               -0.6047\n",
       "               -0.5569\n",
       "               -0.3738\n",
       "                1.4758\n",
       "                1.7546\n",
       "                1.6181\n",
       "               -0.8217\n",
       "               -1.0818\n",
       "               -0.9108\n",
       "               -0.7125\n",
       "                1.3094\n",
       "                0.5921\n",
       "               -0.3303\n",
       "                1.8096\n",
       "                0.2495\n",
       "               -0.8215\n",
       "               -0.6157\n",
       "                3.0933\n",
       "               -0.2032\n",
       "                1.6342\n",
       "                0.7515\n",
       "               -0.6857\n",
       "               -0.2635\n",
       "               -0.2342\n",
       "               -0.3169\n",
       "               -0.2224\n",
       "               -0.6623\n",
       "                2.0715\n",
       "               -0.4874\n",
       "               -0.8139\n",
       "               -0.3648\n",
       "               -0.6237\n",
       "               -1.0920\n",
       "               -0.1697\n",
       "               -0.9860\n",
       "                2.3481\n",
       "                2.7953\n",
       "               -0.5787\n",
       "                1.0621\n",
       "                3.0632\n",
       "               -0.6222\n",
       "               -0.8853\n",
       "                1.1374\n",
       "               -0.4865\n",
       "                2.0065\n",
       "               -0.6479\n",
       "               -0.2191\n",
       "               -0.5428\n",
       "                0.2559\n",
       "               -0.5769\n",
       "                1.3786\n",
       "               -0.2216\n",
       "               -0.5045\n",
       "               -0.6112\n",
       "                1.0355\n",
       "               -0.6947\n",
       "                0.3269\n",
       "               -0.9178\n",
       "               -0.6633\n",
       "               -0.1643\n",
       "               -0.2736\n",
       "               -0.6624\n",
       "               -0.6165\n",
       "               -0.2503\n",
       "                1.4332\n",
       "               -0.6533\n",
       "                0.7812\n",
       "                1.0925\n",
       "               -0.6390\n",
       "               -0.6473\n",
       "               -0.3776\n",
       "               -0.4997\n",
       "                2.8047\n",
       "               -0.3534\n",
       "               -0.1565\n",
       "               -0.7865\n",
       "               -0.7762\n",
       "               -0.9600\n",
       "                2.3574\n",
       "                1.9886\n",
       "               -0.1307\n",
       "               -0.5337\n",
       "               -0.2610\n",
       "               -0.1803\n",
       "               -0.4162\n",
       "               -0.1123\n",
       "               -0.1896\n",
       "               -0.5982\n",
       "                0.4830\n",
       "                2.7772\n",
       "               -0.1615\n",
       "               -1.1318\n",
       "               -0.8179\n",
       "               -0.4698\n",
       "               -0.8436\n",
       "                0.8659\n",
       "               -0.8223\n",
       "                0.8229\n",
       "                0.4794\n",
       "                2.0444\n",
       "               -0.1875\n",
       "               -0.6435\n",
       "                1.6209\n",
       "                0.7525\n",
       "               -1.0385\n",
       "                1.3967\n",
       "                1.1513\n",
       "               -0.7579\n",
       "                0.6712\n",
       "               -0.6510\n",
       "                2.0568\n",
       "                1.2216\n",
       "                0.8786\n",
       "                2.9706\n",
       "               -0.3419\n",
       "                0.4098\n",
       "               -0.7413\n",
       "               -0.5865\n",
       "               -0.0388\n",
       "               -0.7171\n",
       "               -0.3693\n",
       "                2.6941\n",
       "               -0.8199\n",
       "               -0.1188\n",
       "               -0.9607\n",
       "               -0.7726\n",
       "                2.8401\n",
       "                1.4522\n",
       "                1.5335\n",
       "               -0.7196\n",
       "                2.6207\n",
       "               -0.7264\n",
       "               -0.1570\n",
       "               -0.6744\n",
       "                1.9107\n",
       "                1.4272\n",
       "               -0.6116\n",
       "               -0.6205\n",
       "               -0.5670\n",
       "               -0.7548\n",
       "               -0.5709\n",
       "                0.9920\n",
       "               -0.4166\n",
       "                2.2076\n",
       "                0.9605\n",
       "                0.8936\n",
       "               -0.4517\n",
       "                0.8267\n",
       "                2.0503\n",
       "               -0.6259\n",
       "               -0.8647\n",
       "               -0.6096\n",
       "               -0.3182\n",
       "               -0.2069\n",
       "                1.4096\n",
       "                0.7664\n",
       "               -0.8822\n",
       "               -0.1320\n",
       "                2.1412\n",
       "                0.7528\n",
       "               -0.3557\n",
       "               -0.6763\n",
       "               -0.7217\n",
       "                2.7044\n",
       "               -0.5263\n",
       "               -0.7188\n",
       "               -0.7447\n",
       "               -0.7705\n",
       "               -0.6808\n",
       "                1.5445\n",
       "                1.2337\n",
       "               -0.7785\n",
       "               -0.3853\n",
       "                2.3176\n",
       "                0.2874\n",
       "                0.5033\n",
       "               -0.3807\n",
       "               -0.0919\n",
       "                0.8385\n",
       "               -0.2821\n",
       "                1.6212\n",
       "               -0.5775\n",
       "                1.2573\n",
       "                1.7105\n",
       "               -0.6850\n",
       "                1.9908\n",
       "               -0.1538\n",
       "                1.7465\n",
       "                0.0506\n",
       "               -0.1805\n",
       "               -0.4478\n",
       "                2.0648\n",
       "              [torch.FloatTensor of size 512]), ('decode_b.2', \n",
       "              1.00000e-02 *\n",
       "                1.5180\n",
       "                0.7629\n",
       "                0.5477\n",
       "                  ⋮   \n",
       "                0.0548\n",
       "                0.0777\n",
       "                0.0321\n",
       "              [torch.FloatTensor of size 17736])])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rencoder_api.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "  rencoder_api.eval()\n",
    "  rencoder_api = rencoder_api.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_layer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-36c0a77bb9ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmajorInd\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterate_one_epoch_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfor_inf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mtargets_np\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_layer' is not defined"
     ]
    }
   ],
   "source": [
    "    from torch.autograd import Variable \n",
    "    data_api.src_data = data_layer.data\n",
    "    for i, ((out, src), majorInd) in enumerate(data_api.iterate_one_epoch_eval(for_inf=True)):\n",
    "      inputs = Variable(src.cuda().to_dense())\n",
    "      targets_np = out.to_dense().numpy()[0, :]\n",
    "      outputs = rencoder_api(inputs).cpu().data.numpy()[0, :]\n",
    "      non_zeros = targets_np.nonzero()[0].tolist()\n",
    "      print(non_zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "  \"10649\": \"4.1924996\", \r\n",
      "  \"12331\": \"3.3941908\", \r\n",
      "  \"5086\": \"3.1280162\"\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "!curl -X POST -d '{\"13\": \"5.0\",\"191\": \"5.0\", \"209\":\"5.0\"}' -H \"Content-type: application/json\" http://127.0.0.1:5000/recommend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
